{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, DataCollatorForTokenClassification, AutoModelForSequenceClassification, AutoModelForTokenClassification, Trainer, TrainingArguments, pipeline\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import evaluate\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/valListFile.json'\n",
    "test_list_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/testListFile.json'\n",
    "data_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/data.json'\n",
    "dialogue_acts_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/dialogue_acts.json'\n",
    "ontology_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/ontology.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ƒê·ªçc danh s√°ch m√£ h·ªôi tho·∫°i val v√† test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(val_list_path) as f:\n",
    "    val_list = [line.strip() for line in f]\n",
    "    \n",
    "with open(test_list_path) as f:\n",
    "    test_list = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ƒê·ªçc d·ªØ li·ªáu h·ªôi tho·∫°i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path) as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "with open(dialogue_acts_path) as f:\n",
    "    dialogue_acts = json.load(f)\n",
    "    \n",
    "with open(ontology_path) as f:\n",
    "    ontology = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_ids = list(data.keys())\n",
    "\n",
    "train_list = [dialogue_id for dialogue_id in dialogue_ids if dialogue_id not in val_list and dialogue_id not in test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L·∫•y t·∫•t c·∫£ c√°c dialogue acts\n",
    "acts = set()\n",
    "\n",
    "for dialogue in dialogue_acts.values():\n",
    "    for turn in dialogue.values():\n",
    "        if turn == 'No Annotation':\n",
    "            continue\n",
    "        for act in turn.keys():\n",
    "            acts.add(act)\n",
    "            \n",
    "acts.add('No Annotation')\n",
    "acts = list(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë l∆∞·ª£ng acts: 32\n",
      "['Attraction-Request', 'Taxi-Inform', 'Restaurant-Select', 'Train-NoOffer', 'Attraction-Select', 'Train-Select', 'general-bye', 'Booking-Book', 'Hotel-Recommend', 'Restaurant-Inform', 'Restaurant-Recommend', 'Attraction-Inform', 'Train-Request', 'Booking-NoBook', 'Restaurant-NoOffer', 'No Annotation', 'general-greet', 'Train-OfferBook', 'Attraction-NoOffer', 'Hotel-Request', 'Hotel-Inform', 'Hotel-Select', 'Hotel-NoOffer', 'Booking-Request', 'Train-OfferBooked', 'general-welcome', 'Taxi-Request', 'Attraction-Recommend', 'general-reqmore', 'Booking-Inform', 'Train-Inform', 'Restaurant-Request']\n"
     ]
    }
   ],
   "source": [
    "print('S·ªë l∆∞·ª£ng acts:', len(acts))\n",
    "print(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o m·ªôt t·ª´ ƒëi·ªÉn chuy·ªÉn ƒë·ªïi t·ª´ act sang index v√† ng∆∞·ª£c l·∫°i\n",
    "act2idx = {act: idx for idx, act in enumerate(acts)}\n",
    "idx2act = {idx: act for act, idx in act2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danh s√°ch c√°c slot d·∫°ng ph√¢n lo·∫°i v√† kh√¥ng ph√¢n lo·∫°i\n",
    "# Danh s√°ch n√†y ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a theo √Ω hi·ªÉu c√° nh√¢n\n",
    "# V·ªõi bi·∫øn ph√¢n lo·∫°i, ta s·∫Ω chuy·ªÉn th√†nh one-hot vector\n",
    "# V·ªõi bi·∫øn kh√¥ng ph√¢n lo·∫°i, ta s·∫Ω x√°c ƒë·ªãnh span c·ªßa n√≥ trong c√¢u\n",
    "\n",
    "categorical_slots = {\n",
    "    'attraction-area',\n",
    "    'attraction-type',\n",
    "    'bus-day',\n",
    "    'hotel-area',\n",
    "    'hotel-internet',\n",
    "    'hotel-parking',\n",
    "    'hotel-pricerange',\n",
    "    'hotel-book day',\n",
    "    'hotel-stars',\n",
    "    'hotel-type',\n",
    "    'restaurant-area',\n",
    "    'restaurant-book day',\n",
    "    'restaurant-pricerange',\n",
    "    'train-departure',\n",
    "    'train-destination',\n",
    "    'train-day',\n",
    "}\n",
    "\n",
    "non_categorical_slots = {\n",
    "    'attraction-name',\n",
    "    'bus-arriveBy',\n",
    "    'bus-book people',\n",
    "    'bus-departure',\n",
    "    'bus-destination',\n",
    "    'bus-leaveAt',\n",
    "    'hospital-department',\n",
    "    'hotel-book people',\n",
    "    'hotel-book stay',\n",
    "    'hotel-name',\n",
    "    'restaurant-book people',\n",
    "    'restaurant-book time',\n",
    "    'restaurant-food',\n",
    "    'restaurant-name',\n",
    "    'taxi-arriveBy',\n",
    "    'taxi-departure',\n",
    "    'taxi-destination',\n",
    "    'taxi-leaveAt',\n",
    "    'train-arriveBy',\n",
    "    'train-book people',\n",
    "    'train-leaveAt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë l∆∞·ª£ng nh√£n BIO: 43\n",
      "['O', 'B-restaurant-food', 'I-restaurant-food', 'B-attraction-name', 'I-attraction-name', 'B-hospital-department', 'I-hospital-department', 'B-bus-book people', 'I-bus-book people', 'B-train-leaveAt', 'I-train-leaveAt', 'B-train-arriveBy', 'I-train-arriveBy', 'B-taxi-leaveAt', 'I-taxi-leaveAt', 'B-taxi-destination', 'I-taxi-destination', 'B-restaurant-book people', 'I-restaurant-book people', 'B-bus-departure', 'I-bus-departure', 'B-train-book people', 'I-train-book people', 'B-hotel-book stay', 'I-hotel-book stay', 'B-hotel-name', 'I-hotel-name', 'B-taxi-arriveBy', 'I-taxi-arriveBy', 'B-bus-leaveAt', 'I-bus-leaveAt', 'B-hotel-book people', 'I-hotel-book people', 'B-restaurant-name', 'I-restaurant-name', 'B-taxi-departure', 'I-taxi-departure', 'B-bus-arriveBy', 'I-bus-arriveBy', 'B-bus-destination', 'I-bus-destination', 'B-restaurant-book time', 'I-restaurant-book time']\n"
     ]
    }
   ],
   "source": [
    "# T·∫°o nh√£n BIO cho c√°c slot kh√¥ng ph√¢n lo·∫°i\n",
    "bio_list = ['O']\n",
    "bio_list.extend([item for slot in non_categorical_slots for item in [f'B-{slot}', f'I-{slot}']])\n",
    "print('S·ªë l∆∞·ª£ng nh√£n BIO:', len(bio_list))\n",
    "print(bio_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o m·ªôt t·ª´ ƒëi·ªÉn chuy·ªÉn ƒë·ªïi t·ª´ nh√£n BIO sang index v√† ng∆∞·ª£c l·∫°i\n",
    "bio2idx = {bio: idx for idx, bio in enumerate(bio_list)}\n",
    "idx2bio = {idx: bio for bio, idx in bio2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë l∆∞·ª£ng nh√£n value: 173\n",
      "['attraction-type=swimming pool', 'train-day=saturday', 'attraction-area=south', 'train-departure=liverpool', 'restaurant-pricerange=none', 'attraction-type=sports', 'restaurant-book day=thursday', 'restaurant-pricerange=cheap', 'restaurant-area=centre', 'train-day=tuesday', 'hotel-internet=dontcare', 'hotel-area=north', 'train-destination=broxbourne', 'hotel-type=none', 'restaurant-book day=sunday', 'attraction-type=special', 'bus-day=wednesday', 'hotel-area=none', 'attraction-type=college', 'train-destination=huntingdon marriott hotel', 'train-departure=leicester', 'hotel-stars=2', 'hotel-area=south', 'train-departure=brookshite', 'restaurant-area=south', 'train-destination=london', 'hotel-parking=free', 'train-departure=stratford', 'train-departure=huntingdon', 'attraction-type=boating', 'train-departure=east london', 'hotel-book day=tuesday', 'train-departure=panahar', 'restaurant-area=east', 'attraction-type=concert', 'train-departure=birmingham new street', 'train-departure=duxford', 'train-departure=peterborough', 'attraction-type=theatres', 'train-day=monday', 'train-destination=none', 'train-departure=ely', 'train-destination=kings lynn', 'train-departure=city hall', 'hotel-pricerange=expensive', 'restaurant-book day=saturday', 'attraction-type=theatre', 'train-departure=cineworld', 'restaurant-book day=monday', 'hotel-area=east', 'restaurant-book day=tomorrow', 'hotel-internet=yes', 'train-departure=london liverpool', 'restaurant-pricerange=dontcare', 'hotel-area=centre', 'hotel-type=guest house', 'hotel-book day=monday', 'hotel-parking=dontcare', 'train-departure=cambridge', 'train-destination=city centre north', 'train-departure=dontcare', 'attraction-type=camboats', 'restaurant-book day=tuesday', 'train-destination=dontcare', 'restaurant-book day=dontcare', 'train-departure=stansted airport', 'hotel-type=guesthouse', 'train-destination=leicester', 'attraction-type=none', 'hotel-book day=thursday', 'attraction-type=churchills college', 'hotel-stars=dontcare', 'train-destination=london liverpool street', 'hotel-book day=dontcare', 'hotel-pricerange=moderate', 'restaurant-book day=none', 'restaurant-area=north', 'hotel-parking=yes', 'restaurant-pricerange=moderate', 'restaurant-pricerange=expensive', 'attraction-type=park', 'attraction-type=museums', 'train-departure=none', 'train-departure=broxbourne', 'train-destination=liverpool street', 'restaurant-book day=wednesday', 'hotel-stars=5', 'train-departure=stevenage', 'hotel-area=dontcare', 'hotel-book day=saturday', 'bus-day=none', 'attraction-type=architecture', 'attraction-type=dontcare', 'attraction-type=pool', 'hotel-internet=none', 'restaurant-area=dontcare', 'train-day=none', 'restaurant-book day=friday', 'train-destination=london kings cross', 'attraction-area=north', 'hotel-book day=sunday', 'hotel-internet=no', 'train-departure=london liverpool street', 'train-destination=peterborough', 'train-departure=kings lynn', 'attraction-type=gastropub', 'train-destination=stansted airport', 'attraction-type=cinemas', 'attraction-type=nightclub', 'hotel-book day=monday<thursday', 'hotel-type=dontcare', 'train-destination=glastonbury', 'attraction-type=gallery', 'attraction-type=concert hall', 'train-destination=norway', 'hotel-stars=3', 'train-destination=centre', 'hotel-parking=none', 'train-departure=aylesbray lodge guest', 'hotel-stars=none', 'train-destination=ely', 'train-day=dontcare', 'attraction-type=historical', 'train-destination=liverpool', 'hotel-book day=none', 'hotel-pricerange=dontcare', 'hotel-type=hotel', 'hotel-book day=wednesday', 'hotel-parking=no', 'hotel-stars=0', 'restaurant-area=west', 'attraction-type=night club', 'attraction-type=hiking', 'train-destination=bishops stortford', 'train-departure=bishops stortford', 'train-departure=wandlebury country park', 'attraction-area=none', 'attraction-type=multiple sports', 'attraction-type=museum', 'attraction-area=west', 'attraction-type=concerthall', 'hotel-stars=4', 'train-departure=london kings cross', 'train-day=thursday', 'attraction-type=cinema', 'attraction-type=museum kettles yard', 'train-destination=stevenage', 'train-departure=london', 'hotel-pricerange=cheap', 'attraction-type=hotel', 'attraction-area=centre', 'train-destination=bournemouth', 'train-day=sunday', 'train-day=wednesday', 'train-day=tomorrow', 'attraction-type=church', 'restaurant-area=none', 'hotel-pricerange=none', 'attraction-type=entertainment', 'attraction-area=dontcare', 'attraction-area=east', 'train-departure=norwich', 'hotel-stars=1', 'train-destination=norwich', 'hotel-type=bed and breakfast', 'hotel-book day=friday', 'hotel-area=west', 'train-destination=curry prince', 'train-destination=birmingham new street', 'train-day=friday', 'train-destination=cambridge', 'attraction-type=theater', 'attraction-type=boat']\n"
     ]
    }
   ],
   "source": [
    "# T·∫°o nh√£n value cho c√°c slot ph√¢n lo·∫°i\n",
    "categorical_value_list = []\n",
    "\n",
    "for slot in categorical_slots:\n",
    "    for value in ontology[slot]:\n",
    "        for v in re.split(r'\\||>', value):\n",
    "            categorical_value_list.append(f'{slot}={v}')\n",
    "            \n",
    "categorical_value_list = list(set(categorical_value_list))\n",
    "print('S·ªë l∆∞·ª£ng nh√£n value:', len(categorical_value_list))\n",
    "print(categorical_value_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2idx = {cv: idx for idx, cv in enumerate(categorical_value_list)}\n",
    "idx2cv = {idx: cv for cv, idx in cv2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiWozDataset(Dataset):\n",
    "    def __init__(self, dialogue_data, acts_data, tokenizer, act2idx, bio2idx, cv2idx, max_turn=-1, dialogue_ids=None):\n",
    "        self.data = self._process_data(\n",
    "            dialogue_data, acts_data, max_turn, dialogue_ids)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.act2idx = act2idx\n",
    "        self.bio2idx = bio2idx\n",
    "        self.cv2idx = cv2idx\n",
    "        self.problem = 1 # 2, 3\n",
    "\n",
    "    def _process_data(self, dialogue_data, acts_data, max_turn, dialogue_ids):\n",
    "        data = []\n",
    "        for dialogue_id, dialogue in dialogue_data.items():\n",
    "            if dialogue_ids is not None and dialogue_id not in dialogue_ids:\n",
    "                continue\n",
    "            turns = dialogue['log']\n",
    "            history = []\n",
    "            for i in range(0, len(turns) - 1, 2):\n",
    "                user_turn = turns[i]\n",
    "                history.append(user_turn['text'])\n",
    "\n",
    "                system_turn = turns[i + 1]\n",
    "                history.append(system_turn['text'])\n",
    "\n",
    "                # L·∫•y act c·ªßa system\n",
    "                system_acts = acts_data[dialogue_id[:-5]\n",
    "                                        ].get(str(i//2 + 1), 'No Annotation')\n",
    "                if system_acts == 'No Annotation':\n",
    "                    system_acts = ['No Annotation']\n",
    "                else:\n",
    "                    system_acts = list(system_acts.keys())\n",
    "\n",
    "                # L·∫•y slot, value c·ªßa user\n",
    "                slot_values = []\n",
    "                for domain, domain_value in system_turn['metadata'].items():\n",
    "                    for slot, value in domain_value['book'].items():\n",
    "                        if slot == 'booked':\n",
    "                            continue\n",
    "                        if value and value != 'not mentioned':\n",
    "                            slot_values.append(\n",
    "                                [f'{domain}-book {slot}', value])\n",
    "                    for slot, value in domain_value['semi'].items():\n",
    "                        if value and value != 'not mentioned':\n",
    "                            slot_values.append([f'{domain}-{slot}', value])\n",
    "\n",
    "                data.append({\n",
    "                    'dialogue_id': dialogue_id,\n",
    "                    'history': history[max(0, i - 2 * max_turn):i] if max_turn > 0 else history[:i],\n",
    "                    'utterance': user_turn['text'],\n",
    "                    'system_acts': system_acts,\n",
    "                    'slot_values': slot_values\n",
    "                })\n",
    "        return data\n",
    "    \n",
    "    def set_problem(self, problem):\n",
    "        self.problem = problem\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "\n",
    "        history_text = '[SEP]'.join(data['history'])\n",
    "        full_text = f'{history_text}[SEP]{data[\"utterance\"]}' if history_text else data['utterance']\n",
    "\n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(full_text, return_tensors='pt')\n",
    "\n",
    "        if self.problem == 1:\n",
    "            act_labels = torch.zeros(len(self.act2idx))\n",
    "            for act in data['system_acts']:\n",
    "                act_labels[self.act2idx[act]] = 1\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'labels': act_labels\n",
    "            }\n",
    "\n",
    "        if self.problem == 2:\n",
    "            slot_labels = torch.tensor([self.bio2idx['O']] * len(encoding['input_ids'].squeeze(0)))\n",
    "            for slot, value in data['slot_values']:\n",
    "                if slot in non_categorical_slots:\n",
    "                    for v in re.split(r'\\||>', value):\n",
    "                        start, end = self._get_value_start_end(full_text, v)\n",
    "                        if start == 0 and end == 0:\n",
    "                            continue\n",
    "                        slot_labels[start] = self.bio2idx[f'B-{slot}']\n",
    "                        slot_labels[start + 1:end + 1] = self.bio2idx[f'I-{slot}']\n",
    "            # Chuy·ªÉn label c·ªßa [SEP] v√† [CLS] th√†nh -100\n",
    "            sep_idx = (encoding['input_ids'] == self.tokenizer.sep_token_id).nonzero(as_tuple=True)[1]\n",
    "            cls_idx = (encoding['input_ids'] == self.tokenizer.cls_token_id).nonzero(as_tuple=True)[1]\n",
    "            slot_labels[sep_idx] = -100\n",
    "            slot_labels[cls_idx] = -100\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'labels': slot_labels\n",
    "            }\n",
    "            \n",
    "        if self.problem == 3:\n",
    "            categorical_labels = torch.zeros(len(self.cv2idx))\n",
    "            for slot, value in data['slot_values']:\n",
    "                if slot in categorical_slots:\n",
    "                    for v in re.split(r'\\||>', value):\n",
    "                        categorical_labels[self.cv2idx[f'{slot}={v}']] = 1\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'labels': categorical_labels\n",
    "            }\n",
    "\n",
    "    def _get_value_start_end(self, text, value):\n",
    "        tokenized_text = self.tokenizer.tokenize(text, add_special_tokens=True)\n",
    "        tokenized_value = self.tokenizer.tokenize(value)\n",
    "\n",
    "        start, end = 0, 0\n",
    "        for id_v, token_v in enumerate(tokenized_value):\n",
    "            for id_u, token_u in enumerate(tokenized_text):\n",
    "                if token_v == token_u:\n",
    "                    # n·∫øu value ƒë∆∞·ª£c t√¨m th·∫•y trong text\n",
    "                    if tokenized_value == tokenized_text[id_u:id_u+len(tokenized_value)]:\n",
    "                        start, end = id_u, id_u+len(tokenized_value) - 1\n",
    "                        break\n",
    "                    # n·∫øu s·ªë l∆∞·ª£ng token c√≤n l·∫°i trong text √≠t h∆°n s·ªë l∆∞·ª£ng token c·ªßa value\n",
    "                    elif len(tokenized_text) - id_u + 1 <= len(tokenized_value):\n",
    "                        break\n",
    "        return torch.tensor([start, end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultiWozDataset(data, dialogue_acts, tokenizer, act2idx, bio2idx, cv2idx, max_turn=3, dialogue_ids=train_list)\n",
    "val_dataset = MultiWozDataset(data, dialogue_acts, tokenizer, act2idx, bio2idx, cv2idx, max_turn=3, dialogue_ids=val_list)\n",
    "test_dataset = MultiWozDataset(data, dialogue_acts, tokenizer, act2idx, bio2idx, cv2idx, max_turn=3, dialogue_ids=test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M√¥ h√¨nh ph√°t hi·ªán system acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = sigmoid(predictions)\n",
    "    predictions = (predictions > 0.5).astype(int).reshape(-1)\n",
    "    return clf_metrics.compute(predictions=predictions, references=labels.astype(int).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/aircsrv5/miniconda3/envs/imlda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_20906/804699869.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "system_acts_model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = len(acts) , id2label=idx2act, label2id=act2idx, problem_type='multi_label_classification')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "   output_dir=\"model/system_acts_model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=3,\n",
    "   weight_decay=0.01,\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   save_strategy=\"epoch\",\n",
    "   load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "   model=system_acts_model,\n",
    "   args=training_args,\n",
    "   train_dataset=train_dataset,\n",
    "   eval_dataset=val_dataset,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10647' max='10647' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10647/10647 08:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.083200</td>\n",
       "      <td>0.078711</td>\n",
       "      <td>0.968267</td>\n",
       "      <td>0.614339</td>\n",
       "      <td>0.748306</td>\n",
       "      <td>0.521055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.074072</td>\n",
       "      <td>0.969763</td>\n",
       "      <td>0.651492</td>\n",
       "      <td>0.738784</td>\n",
       "      <td>0.582649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>0.073164</td>\n",
       "      <td>0.970076</td>\n",
       "      <td>0.658938</td>\n",
       "      <td>0.736848</td>\n",
       "      <td>0.595929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10647, training_loss=0.08621127546992427, metrics={'train_runtime': 485.6261, 'train_samples_per_second': 350.751, 'train_steps_per_second': 21.924, 'total_flos': 7296220874484864.0, 'train_loss': 0.08621127546992427, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='461' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1/461 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07349257171154022, 'eval_accuracy': 0.9703014785675529, 'eval_f1': 0.6602327837051406, 'eval_precision': 0.7386869234943028, 'eval_recall': 0.5968434896975011, 'eval_runtime': 7.3859, 'eval_samples_per_second': 998.122, 'eval_steps_per_second': 62.416, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Th·ª≠ nghi·ªám tr√™n t·∫≠p test\n",
    "print(trainer.evaluate(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_acts_classifier = pipeline('text-classification', model='model/system_acts_model', device=0, top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need train reservations from norwich to cambridge\n",
      "['Train-Request']\n",
      "I have 133 trains matching your request. Is there a specific day and time you would like to travel?\n",
      "I'd like to leave on Monday and arrive by 18:00.\n",
      "['Train-Request']\n",
      "There are 12 trains for the day and time you request.  Would you like to book it now?\n",
      "Before booking, I would also like to know the travel time, price, and departure time please.\n",
      "['Train-Request', 'Train-Inform']\n",
      "There are 12 trains meeting your needs with the first leaving at 05:16 and the last one leaving at 16:16. Do you want to book one of these? \n",
      "No hold off on booking for now.  Can you help me find an attraction called cineworld cinema?\n",
      "['Attraction-Inform', 'general-reqmore']\n",
      "Yes it is a cinema located in the south part of town what information would you like on it?\n",
      "Yes, that was all I needed. Thank you very much!\n",
      "['general-bye']\n",
      "Thank you for using our system.\n"
     ]
    }
   ],
   "source": [
    "sample_text = [\n",
    "    \"I need train reservations from norwich to cambridge\",\n",
    "    \"I have 133 trains matching your request. Is there a specific day and time you would like to travel?\",\n",
    "    \"I'd like to leave on Monday and arrive by 18:00.\",\n",
    "    \"There are 12 trains for the day and time you request.  Would you like to book it now?\",\n",
    "    \"Before booking, I would also like to know the travel time, price, and departure time please.\",\n",
    "    \"There are 12 trains meeting your needs with the first leaving at 05:16 and the last one leaving at 16:16. Do you want to book one of these? \",\n",
    "    \"No hold off on booking for now.  Can you help me find an attraction called cineworld cinema?\",\n",
    "    \"Yes it is a cinema located in the south part of town what information would you like on it?\",\n",
    "    \"Yes, that was all I needed. Thank you very much!\",\n",
    "    \"Thank you for using our system.\"\n",
    "]\n",
    "\n",
    "history = []\n",
    "max_turn = 3\n",
    "threshold = 0.5\n",
    "for turn in range(0, len(sample_text) - 1, 2):\n",
    "    history_text = '[SEP]'.join(history[max(0, turn - 2 * max_turn):turn])\n",
    "    full_text = f'{history_text}[SEP]{sample_text[turn]}' if history_text else sample_text[turn]\n",
    "    print(sample_text[turn])\n",
    "    print([out['label'] for out in system_acts_classifier(full_text)[0] if out['score'] > threshold])\n",
    "    print(sample_text[turn + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M√¥ h√¨nh ph√°t hi·ªán slot value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V·ªõi slot ph√¢n lo·∫°i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_for_cv = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_problem(3)\n",
    "val_dataset.set_problem(3)\n",
    "test_dataset.set_problem(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/aircsrv5/miniconda3/envs/imlda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_11633/3317102083.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_cv = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17745' max='17745' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17745/17745 14:02, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.052513</td>\n",
       "      <td>0.984724</td>\n",
       "      <td>0.432742</td>\n",
       "      <td>0.825522</td>\n",
       "      <td>0.293227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.043674</td>\n",
       "      <td>0.989007</td>\n",
       "      <td>0.659430</td>\n",
       "      <td>0.857729</td>\n",
       "      <td>0.535603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.027900</td>\n",
       "      <td>0.041521</td>\n",
       "      <td>0.990451</td>\n",
       "      <td>0.716170</td>\n",
       "      <td>0.874680</td>\n",
       "      <td>0.606296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.041931</td>\n",
       "      <td>0.990739</td>\n",
       "      <td>0.727210</td>\n",
       "      <td>0.876831</td>\n",
       "      <td>0.621208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.042428</td>\n",
       "      <td>0.990759</td>\n",
       "      <td>0.729666</td>\n",
       "      <td>0.871303</td>\n",
       "      <td>0.627638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17745, training_loss=0.03783340016093917, metrics={'train_runtime': 843.0354, 'train_samples_per_second': 336.747, 'train_steps_per_second': 21.049, 'total_flos': 1.2184700578628496e+16, 'train_loss': 0.03783340016093917, 'epoch': 5.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_value_model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = len(categorical_value_list), id2label=idx2cv, label2id=cv2idx, problem_type='multi_label_classification')\n",
    "\n",
    "training_args_cv = TrainingArguments(\n",
    "   output_dir=\"model/categorical_value_model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=5,\n",
    "   weight_decay=0.01,\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   save_strategy=\"epoch\",\n",
    "   load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer_cv = Trainer(\n",
    "   model=categorical_value_model,\n",
    "   args=training_args_cv,\n",
    "   train_dataset=train_dataset,\n",
    "   eval_dataset=val_dataset,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator_for_cv,\n",
    "   compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_cv.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_cv.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='461' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/461 00:00 < 00:06, 76.22 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.04028211161494255,\n",
       " 'eval_accuracy': 0.9904842255809359,\n",
       " 'eval_f1': 0.7133138051592176,\n",
       " 'eval_precision': 0.8695501929390083,\n",
       " 'eval_recall': 0.6046697905402699,\n",
       " 'eval_runtime': 13.4662,\n",
       " 'eval_samples_per_second': 547.445,\n",
       " 'eval_steps_per_second': 34.234,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_cv.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V·ªõi slot kh√¥ng ph√¢n lo·∫°i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_problem(2)\n",
    "val_dataset.set_problem(2)\n",
    "test_dataset.set_problem(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics_tf(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [idx2bio[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [idx2bio[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "non_categorical_value_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=len(bio_list), id2label=idx2bio, label2id=bio2idx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17745' max='17745' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17745/17745 16:00, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.022353</td>\n",
       "      <td>0.841603</td>\n",
       "      <td>0.865539</td>\n",
       "      <td>0.853403</td>\n",
       "      <td>0.992461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.020116</td>\n",
       "      <td>0.862204</td>\n",
       "      <td>0.873999</td>\n",
       "      <td>0.868061</td>\n",
       "      <td>0.993115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.019766</td>\n",
       "      <td>0.881547</td>\n",
       "      <td>0.908919</td>\n",
       "      <td>0.895024</td>\n",
       "      <td>0.994007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.022401</td>\n",
       "      <td>0.887569</td>\n",
       "      <td>0.894519</td>\n",
       "      <td>0.891031</td>\n",
       "      <td>0.993705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.023972</td>\n",
       "      <td>0.887765</td>\n",
       "      <td>0.892719</td>\n",
       "      <td>0.890235</td>\n",
       "      <td>0.993597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aircsrv5/miniconda3/envs/imlda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aircsrv5/miniconda3/envs/imlda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17745, training_loss=0.02148783508573797, metrics={'train_runtime': 960.2125, 'train_samples_per_second': 295.653, 'train_steps_per_second': 18.48, 'total_flos': 1.199016127480728e+16, 'train_loss': 0.02148783508573797, 'epoch': 5.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args_ncv = TrainingArguments(\n",
    "    output_dir=\"model/non_categorical_value_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer_ncv = Trainer(\n",
    "    model=non_categorical_value_model,\n",
    "    args=training_args_ncv,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics_tf,\n",
    ")\n",
    "\n",
    "trainer_ncv.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ncv.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='461' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1/461 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.018289389088749886,\n",
       " 'eval_precision': 0.8839788732394366,\n",
       " 'eval_recall': 0.91565605908635,\n",
       " 'eval_f1': 0.8995386751466834,\n",
       " 'eval_accuracy': 0.9945015186900702,\n",
       " 'eval_runtime': 13.8267,\n",
       " 'eval_samples_per_second': 533.173,\n",
       " 'eval_steps_per_second': 33.341,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ncv.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_value_clf = pipeline('text-classification', model='model/categorical_value_model', device=0, top_k=None)\n",
    "non_categorical_value_tclf = pipeline('ner', model='model/non_categorical_value_model', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slot_vlaue_predict(full_text):\n",
    "    state = defaultdict(list)\n",
    "    categorical_value_result = categorical_value_clf(full_text)\n",
    "    non_categorical_value_result = non_categorical_value_tclf(full_text)\n",
    "    \n",
    "    for out in categorical_value_result[0]:\n",
    "        if out['score'] > 0.5:\n",
    "            slot, value = out['label'].split('=')\n",
    "            state[slot].append(value)\n",
    "            \n",
    "    current_entity = None\n",
    "    current_value = \"\"\n",
    "    \n",
    "    for item in non_categorical_value_result:\n",
    "        entity_type = item['entity'][2:]  # Remove the B- or I- prefix\n",
    "        if item['entity'].startswith('B-'):\n",
    "            if current_entity:  # Save the previous entity-value pair if exists\n",
    "                if current_value.find(':') != -1:\n",
    "                    current_value = current_value.replace(' ', '')\n",
    "                state[current_entity].append(current_value)\n",
    "            current_entity = entity_type\n",
    "            current_value = item['word']\n",
    "        elif item['entity'].startswith('I-') and current_entity == entity_type:\n",
    "            if item['word'].startswith('##'):\n",
    "                current_value += item['word'][2:]\n",
    "            else:\n",
    "                current_value += ' ' + item['word']  # Concatenate words for the same entity\n",
    "\n",
    "    # Append the last entity-value pair\n",
    "    if current_entity:\n",
    "        if current_value.find(':') != -1:\n",
    "            current_value = current_value.replace(' ', '')\n",
    "        state[current_entity].append(current_value)\n",
    "        \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need train reservations from norwich to cambridge\n",
      "{'train-destination': ['cambridge'], 'train-departure': ['norwich']}\n",
      "I have 133 trains matching your request. Is there a specific day and time you would like to travel?\n",
      "I'd like to leave on Monday and arrive by 18:00.\n",
      "{'train-destination': ['cambridge'], 'train-departure': ['norwich'], 'train-day': ['monday'], 'train-arriveBy': ['18:00']}\n",
      "There are 12 trains for the day and time you request.  Would you like to book it now?\n",
      "Before booking, I would also like to know the travel time, price, and departure time please.\n",
      "{'train-destination': ['cambridge'], 'train-departure': ['cambridge'], 'train-day': ['monday'], 'train-arriveBy': ['18:00']}\n",
      "There are 12 trains meeting your needs with the first leaving at 05:16 and the last one leaving at 16:16. Do you want to book one of these? \n",
      "No hold off on booking for now.  Can you help me find an attraction called cineworld cinema?\n",
      "{'train-destination': ['cambridge'], 'train-departure': ['cambridge'], 'train-day': ['monday'], 'train-arriveBy': ['18:00'], 'attraction-name': ['cineworld cinema']}\n",
      "Yes it is a cinema located in the south part of town what information would you like on it?\n",
      "Yes, that was all I needed. Thank you very much!\n",
      "{'train-destination': ['cambridge'], 'train-departure': ['cambridge'], 'train-day': ['monday'], 'train-arriveBy': ['18:00'], 'attraction-name': ['cineworld cinema']}\n",
      "Thank you for using our system.\n"
     ]
    }
   ],
   "source": [
    "state = {}\n",
    "max_turn = 3\n",
    "for turn in range(0, len(sample_text) - 1, 2):\n",
    "    history_text = '[SEP]'.join(history[max(0, turn - 2 * max_turn):turn])\n",
    "    full_text = f'{history_text}[SEP]{sample_text[turn]}' if history_text else sample_text[turn]\n",
    "    print(sample_text[turn])\n",
    "    state = dict(state | slot_vlaue_predict(full_text))\n",
    "    print(state)\n",
    "    print(sample_text[turn + 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
