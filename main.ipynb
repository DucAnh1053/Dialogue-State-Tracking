{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, DataCollatorForTokenClassification, AutoModelForSequenceClassification, AutoModelForTokenClassification, Trainer, TrainingArguments, pipeline\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import evaluate\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/valListFile.json'\n",
    "test_list_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/testListFile.json'\n",
    "data_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/data.json'\n",
    "dialogue_acts_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/dialogue_acts.json'\n",
    "ontology_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/ontology.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đọc danh sách mã hội thoại val và test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(val_list_path) as f:\n",
    "    val_list = [line.strip() for line in f]\n",
    "    \n",
    "with open(test_list_path) as f:\n",
    "    test_list = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đọc dữ liệu hội thoại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path) as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "with open(dialogue_acts_path) as f:\n",
    "    dialogue_acts = json.load(f)\n",
    "    \n",
    "with open(ontology_path) as f:\n",
    "    ontology = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_ids = list(data.keys())\n",
    "\n",
    "train_list = [dialogue_id for dialogue_id in dialogue_ids if dialogue_id not in val_list and dialogue_id not in test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy tất cả các dialogue acts\n",
    "acts = set()\n",
    "\n",
    "for dialogue in dialogue_acts.values():\n",
    "    for turn in dialogue.values():\n",
    "        if turn == 'No Annotation':\n",
    "            continue\n",
    "        for act in turn.keys():\n",
    "            acts.add(act)\n",
    "            \n",
    "acts.add('No Annotation')\n",
    "acts = list(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng acts: 32\n",
      "['Restaurant-NoOffer', 'Taxi-Inform', 'No Annotation', 'Attraction-Recommend', 'Restaurant-Select', 'Train-Select', 'Train-Request', 'general-greet', 'Restaurant-Inform', 'general-reqmore', 'Booking-Inform', 'Hotel-Inform', 'general-bye', 'Hotel-Select', 'Booking-Request', 'Train-NoOffer', 'Attraction-Request', 'Booking-Book', 'Train-Inform', 'general-welcome', 'Taxi-Request', 'Booking-NoBook', 'Restaurant-Recommend', 'Hotel-Recommend', 'Attraction-Select', 'Attraction-Inform', 'Hotel-Request', 'Hotel-NoOffer', 'Attraction-NoOffer', 'Train-OfferBook', 'Restaurant-Request', 'Train-OfferBooked']\n"
     ]
    }
   ],
   "source": [
    "print('Số lượng acts:', len(acts))\n",
    "print(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo một từ điển chuyển đổi từ act sang index và ngược lại\n",
    "act2idx = {act: idx for idx, act in enumerate(acts)}\n",
    "idx2act = {idx: act for act, idx in act2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danh sách các slot dạng phân loại và không phân loại\n",
    "# Danh sách này được định nghĩa theo ý hiểu cá nhân\n",
    "# Với biến phân loại, ta sẽ chuyển thành one-hot vector\n",
    "# Với biến không phân loại, ta sẽ xác định span của nó trong câu\n",
    "\n",
    "categorical_slots = {\n",
    "    'attraction-area',\n",
    "    'attraction-type',\n",
    "    'bus-day',\n",
    "    'hotel-area',\n",
    "    'hotel-internet',\n",
    "    'hotel-parking',\n",
    "    'hotel-pricerange',\n",
    "    'hotel-stars',\n",
    "    'hotel-type',\n",
    "    'restaurant-area',\n",
    "    'restaurant-pricerange',\n",
    "    'train-departure',\n",
    "    'train-destination',\n",
    "}\n",
    "\n",
    "non_categorical_slots = {\n",
    "    'attraction-name',\n",
    "    'bus-arriveBy',\n",
    "    'bus-book people',\n",
    "    'bus-departure',\n",
    "    'bus-destination',\n",
    "    'bus-leaveAt',\n",
    "    'hospital-department',\n",
    "    'hotel-book day',\n",
    "    'hotel-book people',\n",
    "    'hotel-book stay',\n",
    "    'hotel-name',\n",
    "    'restaurant-book day',\n",
    "    'restaurant-book people',\n",
    "    'restaurant-book time',\n",
    "    'restaurant-food',\n",
    "    'restaurant-name',\n",
    "    'taxi-arriveBy',\n",
    "    'taxi-departure',\n",
    "    'taxi-destination',\n",
    "    'taxi-leaveAt',\n",
    "    'train-arriveBy',\n",
    "    'train-book people',\n",
    "    'train-day',\n",
    "    'train-leaveAt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng nhãn BIO: 49\n",
      "['O', 'B-taxi-leaveAt', 'I-taxi-leaveAt', 'B-restaurant-book time', 'I-restaurant-book time', 'B-attraction-name', 'I-attraction-name', 'B-bus-arriveBy', 'I-bus-arriveBy', 'B-taxi-departure', 'I-taxi-departure', 'B-bus-departure', 'I-bus-departure', 'B-bus-leaveAt', 'I-bus-leaveAt', 'B-hotel-name', 'I-hotel-name', 'B-taxi-destination', 'I-taxi-destination', 'B-taxi-arriveBy', 'I-taxi-arriveBy', 'B-restaurant-name', 'I-restaurant-name', 'B-bus-book people', 'I-bus-book people', 'B-restaurant-food', 'I-restaurant-food', 'B-train-book people', 'I-train-book people', 'B-hospital-department', 'I-hospital-department', 'B-restaurant-book day', 'I-restaurant-book day', 'B-hotel-book day', 'I-hotel-book day', 'B-train-day', 'I-train-day', 'B-train-leaveAt', 'I-train-leaveAt', 'B-train-arriveBy', 'I-train-arriveBy', 'B-hotel-book people', 'I-hotel-book people', 'B-restaurant-book people', 'I-restaurant-book people', 'B-hotel-book stay', 'I-hotel-book stay', 'B-bus-destination', 'I-bus-destination']\n"
     ]
    }
   ],
   "source": [
    "# Tạo nhãn BIO cho các slot không phân loại\n",
    "bio_list = ['O']\n",
    "bio_list.extend([item for slot in non_categorical_slots for item in [f'B-{slot}', f'I-{slot}']])\n",
    "print('Số lượng nhãn BIO:', len(bio_list))\n",
    "print(bio_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo một từ điển chuyển đổi từ nhãn BIO sang index và ngược lại\n",
    "bio2idx = {bio: idx for idx, bio in enumerate(bio_list)}\n",
    "idx2bio = {idx: bio for bio, idx in bio2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng nhãn value: 143\n",
      "['train-departure none', 'hotel-area north', 'hotel-parking dontcare', 'train-destination london kings cross', 'hotel-pricerange moderate', 'hotel-pricerange none', 'attraction-type gastropub', 'hotel-area centre', 'attraction-type cinema', 'attraction-type museums', 'hotel-area dontcare', 'restaurant-area none', 'train-destination huntingdon marriott hotel', 'train-departure london liverpool', 'attraction-type hotel', 'train-destination curry prince', 'attraction-area south', 'train-departure norwich', 'hotel-type hotel', 'attraction-type concert hall', 'train-destination broxbourne', 'hotel-type guesthouse', 'train-destination kings lynn', 'attraction-type historical', 'attraction-type church', 'restaurant-area north', 'restaurant-area dontcare', 'hotel-pricerange expensive', 'hotel-internet dontcare', 'train-departure liverpool', 'attraction-type theatre', 'attraction-type dontcare', 'hotel-stars 1', 'train-destination stansted airport', 'restaurant-area west', 'hotel-stars 5', 'train-departure broxbourne', 'train-departure aylesbray lodge guest', 'attraction-type theater', 'attraction-type concert', 'train-departure stevenage', 'train-departure brookshite', 'attraction-area west', 'restaurant-area south', 'attraction-type gallery', 'train-departure birmingham new street', 'train-departure london liverpool street', 'hotel-area west', 'train-destination london liverpool street', 'hotel-pricerange cheap', 'bus-day none', 'hotel-stars none', 'attraction-type swimming pool', 'train-destination leicester', 'hotel-internet none', 'train-destination centre', 'attraction-type nightclub', 'restaurant-pricerange none', 'attraction-area centre', 'attraction-area north', 'train-destination city centre north', 'train-departure bishops stortford', 'attraction-type special', 'attraction-type camboats', 'train-departure dontcare', 'hotel-type none', 'attraction-type multiple sports', 'train-departure peterborough', 'hotel-area south', 'train-departure city hall', 'hotel-stars dontcare', 'train-destination glastonbury', 'hotel-pricerange dontcare', 'train-departure cambridge', 'bus-day wednesday', 'hotel-stars 0', 'train-destination ely', 'attraction-type museum kettles yard', 'train-destination norway', 'train-destination bishops stortford', 'restaurant-area centre', 'train-departure leicester', 'attraction-area dontcare', 'train-departure cineworld', 'hotel-internet yes', 'train-departure stansted airport', 'hotel-area east', 'attraction-type museum', 'train-departure wandlebury country park', 'train-departure huntingdon', 'attraction-type theatres', 'hotel-stars 3', 'hotel-type guest house', 'train-destination liverpool street', 'train-destination peterborough', 'attraction-type boat', 'attraction-type none', 'attraction-type pool', 'train-destination liverpool', 'hotel-stars 4', 'hotel-internet no', 'attraction-type night club', 'hotel-type bed and breakfast', 'attraction-area none', 'attraction-type entertainment', 'hotel-parking no', 'train-departure panahar', 'attraction-type hiking', 'train-departure kings lynn', 'hotel-type dontcare', 'train-destination dontcare', 'train-destination stevenage', 'train-departure london kings cross', 'train-departure ely', 'hotel-parking none', 'restaurant-pricerange expensive', 'restaurant-pricerange cheap', 'train-destination bournemouth', 'train-destination norwich', 'restaurant-pricerange moderate', 'hotel-stars 2', 'attraction-area east', 'attraction-type concerthall', 'attraction-type sports', 'attraction-type architecture', 'hotel-parking free', 'attraction-type college', 'train-departure east london', 'train-departure stratford', 'train-destination cambridge', 'hotel-area none', 'hotel-parking yes', 'train-departure duxford', 'train-departure london', 'restaurant-pricerange dontcare', 'train-destination london', 'restaurant-area east', 'attraction-type park', 'attraction-type cinemas', 'attraction-type churchills college', 'attraction-type boating', 'train-destination birmingham new street', 'train-destination none']\n"
     ]
    }
   ],
   "source": [
    "# Tạo nhãn value cho các slot phân loại\n",
    "categorical_value_list = []\n",
    "\n",
    "for slot in categorical_slots:\n",
    "    for value in ontology[slot]:\n",
    "        for v in re.split(r'\\||>', value):\n",
    "            categorical_value_list.append(f'{slot} {v}')\n",
    "            \n",
    "categorical_value_list = list(set(categorical_value_list))\n",
    "print('Số lượng nhãn value:', len(categorical_value_list))\n",
    "print(categorical_value_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2idx = {cv: idx for idx, cv in enumerate(categorical_value_list)}\n",
    "idx2cv = {idx: cv for cv, idx in cv2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiWozDataset(Dataset):\n",
    "    def __init__(self, dialogue_data, acts_data, tokenizer, act2idx, bio2idx, cv2idx, max_turn=-1, dialogue_ids=None):\n",
    "        self.data = self._process_data(\n",
    "            dialogue_data, acts_data, max_turn, dialogue_ids)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.act2idx = act2idx\n",
    "        self.bio2idx = bio2idx\n",
    "        self.cv2idx = cv2idx\n",
    "        self.problem = 1 # 2, 3\n",
    "\n",
    "    def _process_data(self, dialogue_data, acts_data, max_turn, dialogue_ids):\n",
    "        data = []\n",
    "        for dialogue_id, dialogue in dialogue_data.items():\n",
    "            if dialogue_ids is not None and dialogue_id not in dialogue_ids:\n",
    "                continue\n",
    "            turns = dialogue['log']\n",
    "            history = []\n",
    "            for i in range(0, len(turns) - 1, 2):\n",
    "                user_turn = turns[i]\n",
    "                history.append(user_turn['text'])\n",
    "\n",
    "                system_turn = turns[i + 1]\n",
    "                history.append(system_turn['text'])\n",
    "\n",
    "                # Lấy act của system\n",
    "                system_acts = acts_data[dialogue_id[:-5]\n",
    "                                        ].get(str(i//2 + 1), 'No Annotation')\n",
    "                if system_acts == 'No Annotation':\n",
    "                    system_acts = ['No Annotation']\n",
    "                else:\n",
    "                    system_acts = list(system_acts.keys())\n",
    "\n",
    "                # Lấy slot, value của user\n",
    "                slot_values = []\n",
    "                for domain, domain_value in system_turn['metadata'].items():\n",
    "                    for slot, value in domain_value['book'].items():\n",
    "                        if slot == 'booked':\n",
    "                            continue\n",
    "                        if value and value != 'not mentioned':\n",
    "                            slot_values.append(\n",
    "                                [f'{domain}-book {slot}', value])\n",
    "                    for slot, value in domain_value['semi'].items():\n",
    "                        if value and value != 'not mentioned':\n",
    "                            slot_values.append([f'{domain}-{slot}', value])\n",
    "\n",
    "                data.append({\n",
    "                    'dialogue_id': dialogue_id,\n",
    "                    'history': history[max(0, i - 2 * max_turn):i] if max_turn > 0 else history[:i],\n",
    "                    'utterance': user_turn['text'],\n",
    "                    'system_acts': system_acts,\n",
    "                    'slot_values': slot_values\n",
    "                })\n",
    "        return data\n",
    "    \n",
    "    def set_problem(self, problem):\n",
    "        self.problem = problem\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "\n",
    "        history_text = '[SEP]'.join(data['history'])\n",
    "        full_text = f'{history_text}[SEP]{data[\"utterance\"]}' if history_text else data['utterance']\n",
    "\n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(full_text, return_tensors='pt')\n",
    "\n",
    "        if self.problem == 1:\n",
    "            act_labels = torch.zeros(len(self.act2idx))\n",
    "            for act in data['system_acts']:\n",
    "                act_labels[self.act2idx[act]] = 1\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'labels': act_labels\n",
    "            }\n",
    "\n",
    "        if self.problem == 2:\n",
    "            slot_labels = torch.tensor([self.bio2idx['O']] * len(encoding['input_ids'].squeeze(0)))\n",
    "            for slot, value in data['slot_values']:\n",
    "                if slot in non_categorical_slots:\n",
    "                    for v in re.split(r'\\||>', value):\n",
    "                        start, end = self._get_value_start_end(full_text, v)\n",
    "                        if start == 0 and end == 0:\n",
    "                            continue\n",
    "                        slot_labels[start] = self.bio2idx[f'B-{slot}']\n",
    "                        slot_labels[start + 1:end + 1] = self.bio2idx[f'I-{slot}']\n",
    "            # Chuyển label của [SEP] và [CLS] thành -100\n",
    "            sep_idx = (encoding['input_ids'] == self.tokenizer.sep_token_id).nonzero(as_tuple=True)[1]\n",
    "            cls_idx = (encoding['input_ids'] == self.tokenizer.cls_token_id).nonzero(as_tuple=True)[1]\n",
    "            slot_labels[sep_idx] = -100\n",
    "            slot_labels[cls_idx] = -100\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'labels': slot_labels\n",
    "            }\n",
    "            \n",
    "        if self.problem == 3:\n",
    "            categorical_labels = torch.zeros(len(self.cv2idx))\n",
    "            for slot, value in data['slot_values']:\n",
    "                if slot in categorical_slots:\n",
    "                    for v in re.split(r'\\||>', value):\n",
    "                        categorical_labels[self.cv2idx[f'{slot} {v}']] = 1\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'labels': categorical_labels\n",
    "            }\n",
    "\n",
    "    def _get_value_start_end(self, text, value):\n",
    "        tokenized_text = self.tokenizer.tokenize(text, add_special_tokens=True)\n",
    "        tokenized_value = self.tokenizer.tokenize(value)\n",
    "\n",
    "        start, end = 0, 0\n",
    "        for id_v, token_v in enumerate(tokenized_value):\n",
    "            for id_u, token_u in enumerate(tokenized_text):\n",
    "                if token_v == token_u:\n",
    "                    # nếu value được tìm thấy trong text\n",
    "                    if tokenized_value == tokenized_text[id_u:id_u+len(tokenized_value)]:\n",
    "                        start, end = id_u, id_u+len(tokenized_value) - 1\n",
    "                        break\n",
    "                    # nếu số lượng token còn lại trong text ít hơn số lượng token của value\n",
    "                    elif len(tokenized_text) - id_u + 1 <= len(tokenized_value):\n",
    "                        break\n",
    "        return torch.tensor([start, end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultiWozDataset(data, dialogue_acts, tokenizer, act2idx, bio2idx, cv2idx, max_turn=3, dialogue_ids=train_list)\n",
    "val_dataset = MultiWozDataset(data, dialogue_acts, tokenizer, act2idx, bio2idx, cv2idx, max_turn=3, dialogue_ids=val_list)\n",
    "test_dataset = MultiWozDataset(data, dialogue_acts, tokenizer, act2idx, bio2idx, cv2idx, max_turn=3, dialogue_ids=test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mô hình phát hiện system acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = sigmoid(predictions)\n",
    "    predictions = (predictions > 0.5).astype(int).reshape(-1)\n",
    "    return clf_metrics.compute(predictions=predictions, references=labels.astype(int).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/aircsrv5/miniconda3/envs/imlda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_44333/804699869.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "system_acts_model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = len(acts) , id2label=idx2act, label2id=act2idx, problem_type='multi_label_classification')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "   output_dir=\"model/system_acts_model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=3,\n",
    "   weight_decay=0.01,\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   save_strategy=\"epoch\",\n",
    "   load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "   model=system_acts_model,\n",
    "   args=training_args,\n",
    "   train_dataset=train_dataset,\n",
    "   eval_dataset=val_dataset,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10647' max='10647' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10647/10647 08:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.083300</td>\n",
       "      <td>0.078936</td>\n",
       "      <td>0.968127</td>\n",
       "      <td>0.611298</td>\n",
       "      <td>0.748323</td>\n",
       "      <td>0.516687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.078200</td>\n",
       "      <td>0.074162</td>\n",
       "      <td>0.969941</td>\n",
       "      <td>0.649953</td>\n",
       "      <td>0.746853</td>\n",
       "      <td>0.575310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.073100</td>\n",
       "      <td>0.073017</td>\n",
       "      <td>0.970237</td>\n",
       "      <td>0.656728</td>\n",
       "      <td>0.745368</td>\n",
       "      <td>0.586930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10647, training_loss=0.08661638990617015, metrics={'train_runtime': 495.9088, 'train_samples_per_second': 343.478, 'train_steps_per_second': 21.47, 'total_flos': 7296220874484864.0, 'train_loss': 0.08661638990617015, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='461' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/461 00:00 < 00:06, 76.42 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07333177328109741, 'eval_accuracy': 0.9704625610417797, 'eval_f1': 0.6582973715182424, 'eval_precision': 0.7468565706019806, 'eval_recall': 0.5885138097325734, 'eval_runtime': 7.5121, 'eval_samples_per_second': 981.345, 'eval_steps_per_second': 61.367, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Thử nghiệm trên tập test\n",
    "print(trainer.evaluate(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_acts_classifier = pipeline('text-classification', model='model/system_acts_model', device=0, top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need train reservations from norwich to cambridge\n",
      "['Train-Request']\n",
      "I have 133 trains matching your request. Is there a specific day and time you would like to travel?\n",
      "I'd like to leave on Monday and arrive by 18:00.\n",
      "['Train-Request']\n",
      "There are 12 trains for the day and time you request.  Would you like to book it now?\n",
      "Before booking, I would also like to know the travel time, price, and departure time please.\n",
      "['Train-Inform']\n",
      "There are 12 trains meeting your needs with the first leaving at 05:16 and the last one leaving at 16:16. Do you want to book one of these? \n",
      "No hold off on booking for now.  Can you help me find an attraction called cineworld cinema?\n",
      "['Attraction-Inform', 'general-reqmore']\n",
      "Yes it is a cinema located in the south part of town what information would you like on it?\n",
      "Yes, that was all I needed. Thank you very much!\n",
      "['general-bye']\n",
      "Thank you for using our system.\n"
     ]
    }
   ],
   "source": [
    "sample_text = [\n",
    "    \"I need train reservations from norwich to cambridge\",\n",
    "    \"I have 133 trains matching your request. Is there a specific day and time you would like to travel?\",\n",
    "    \"I'd like to leave on Monday and arrive by 18:00.\",\n",
    "    \"There are 12 trains for the day and time you request.  Would you like to book it now?\",\n",
    "    \"Before booking, I would also like to know the travel time, price, and departure time please.\",\n",
    "    \"There are 12 trains meeting your needs with the first leaving at 05:16 and the last one leaving at 16:16. Do you want to book one of these? \",\n",
    "    \"No hold off on booking for now.  Can you help me find an attraction called cineworld cinema?\",\n",
    "    \"Yes it is a cinema located in the south part of town what information would you like on it?\",\n",
    "    \"Yes, that was all I needed. Thank you very much!\",\n",
    "    \"Thank you for using our system.\"\n",
    "]\n",
    "\n",
    "history = []\n",
    "max_turn = 3\n",
    "threshold = 0.5\n",
    "for turn in range(0, len(sample_text) - 1, 2):\n",
    "    history_text = '[SEP]'.join(history[max(0, turn - 2 * max_turn):turn])\n",
    "    full_text = f'{history_text}[SEP]{sample_text[turn]}' if history_text else sample_text[turn]\n",
    "    print(sample_text[turn])\n",
    "    print([out['label'] for out in system_acts_classifier(full_text)[0] if out['score'] > threshold])\n",
    "    print(sample_text[turn + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mô hình phát hiện slot value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Với slot phân loại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_for_cv = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_problem(3)\n",
    "val_dataset.set_problem(3)\n",
    "test_dataset.set_problem(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/aircsrv5/miniconda3/envs/imlda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_44333/3317102083.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_cv = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17745' max='17745' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17745/17745 14:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.049127</td>\n",
       "      <td>0.985502</td>\n",
       "      <td>0.487118</td>\n",
       "      <td>0.774152</td>\n",
       "      <td>0.355360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.042078</td>\n",
       "      <td>0.989745</td>\n",
       "      <td>0.686968</td>\n",
       "      <td>0.840606</td>\n",
       "      <td>0.580813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.041252</td>\n",
       "      <td>0.990556</td>\n",
       "      <td>0.716696</td>\n",
       "      <td>0.855600</td>\n",
       "      <td>0.616593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.042296</td>\n",
       "      <td>0.990708</td>\n",
       "      <td>0.723673</td>\n",
       "      <td>0.853740</td>\n",
       "      <td>0.627998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.021500</td>\n",
       "      <td>0.043085</td>\n",
       "      <td>0.990738</td>\n",
       "      <td>0.726115</td>\n",
       "      <td>0.850043</td>\n",
       "      <td>0.633725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17745, training_loss=0.03461599121566691, metrics={'train_runtime': 853.9469, 'train_samples_per_second': 332.445, 'train_steps_per_second': 20.78, 'total_flos': 1.2178201492754736e+16, 'train_loss': 0.03461599121566691, 'epoch': 5.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_value_model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = len(categorical_value_list), id2label=idx2cv, label2id=cv2idx, problem_type='multi_label_classification')\n",
    "\n",
    "training_args_cv = TrainingArguments(\n",
    "   output_dir=\"model/categorical_value_model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=5,\n",
    "   weight_decay=0.01,\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   save_strategy=\"epoch\",\n",
    "   load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer_cv = Trainer(\n",
    "   model=categorical_value_model,\n",
    "   args=training_args_cv,\n",
    "   train_dataset=train_dataset,\n",
    "   eval_dataset=val_dataset,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator_for_cv,\n",
    "   compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_cv.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_cv.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='461' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/461 00:00 < 00:06, 74.55 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.03925233706831932,\n",
       " 'eval_accuracy': 0.9907237363829876,\n",
       " 'eval_f1': 0.7162134710815752,\n",
       " 'eval_precision': 0.8506238367684567,\n",
       " 'eval_recall': 0.6184843624699278,\n",
       " 'eval_runtime': 12.2403,\n",
       " 'eval_samples_per_second': 602.273,\n",
       " 'eval_steps_per_second': 37.662,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_cv.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Với slot không phân loại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_problem(2)\n",
    "val_dataset.set_problem(2)\n",
    "test_dataset.set_problem(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics_tf(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [idx2bio[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [idx2bio[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "non_categorical_value_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=len(bio_list), id2label=idx2bio, label2id=bio2idx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10647' max='10647' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10647/10647 10:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.022622</td>\n",
       "      <td>0.862630</td>\n",
       "      <td>0.888311</td>\n",
       "      <td>0.875282</td>\n",
       "      <td>0.992512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>0.020227</td>\n",
       "      <td>0.884951</td>\n",
       "      <td>0.902272</td>\n",
       "      <td>0.893528</td>\n",
       "      <td>0.993210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.020228</td>\n",
       "      <td>0.893529</td>\n",
       "      <td>0.906310</td>\n",
       "      <td>0.899874</td>\n",
       "      <td>0.993514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aircsrv5/miniconda3/envs/imlda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aircsrv5/miniconda3/envs/imlda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10647, training_loss=0.03109562422755157, metrics={'train_runtime': 606.6708, 'train_samples_per_second': 280.768, 'train_steps_per_second': 17.55, 'total_flos': 7198554702861252.0, 'train_loss': 0.03109562422755157, 'epoch': 3.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args_ncv = TrainingArguments(\n",
    "    output_dir=\"model/non_categorical_value_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer_ncv = Trainer(\n",
    "    model=non_categorical_value_model,\n",
    "    args=training_args_ncv,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics_tf,\n",
    ")\n",
    "\n",
    "trainer_ncv.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ncv.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='461' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1/461 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.019612910225987434,\n",
       " 'eval_precision': 0.8848949919224556,\n",
       " 'eval_recall': 0.9061207609594706,\n",
       " 'eval_f1': 0.8953821005312628,\n",
       " 'eval_accuracy': 0.9936638199352084,\n",
       " 'eval_runtime': 15.0322,\n",
       " 'eval_samples_per_second': 490.415,\n",
       " 'eval_steps_per_second': 30.668,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ncv.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_value_clf = pipeline('text-classification', model='model/categorical_value_model', device=0, top_k=None)\n",
    "non_categorical_value_tclf = pipeline('ner', model='model/non_categorical_value_model', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slot_vlaue_predict(full_text):\n",
    "    state = defaultdict(list)\n",
    "    categorical_value_result = categorical_value_clf(full_text)\n",
    "    non_categorical_value_result = non_categorical_value_tclf(full_text)\n",
    "    \n",
    "    for out in categorical_value_result[0]:\n",
    "        if out['score'] > 0.5:\n",
    "            slot, value = out['label'].split(' ', 1)\n",
    "            state[slot].append(value)\n",
    "            \n",
    "    current_entity = None\n",
    "    current_value = \"\"\n",
    "    \n",
    "    for item in non_categorical_value_result:\n",
    "        entity_type = item['entity'][2:]  # Remove the B- or I- prefix\n",
    "        if item['entity'].startswith('B-'):\n",
    "            if current_entity:  # Save the previous entity-value pair if exists\n",
    "                if current_value.find(':') != -1:\n",
    "                    current_value = current_value.replace(' ', '')\n",
    "                state[current_entity].append(current_value)\n",
    "            current_entity = entity_type\n",
    "            current_value = item['word']\n",
    "        elif item['entity'].startswith('I-') and current_entity == entity_type:\n",
    "            if item['word'].startswith('##'):\n",
    "                current_value += item['word'][2:]\n",
    "            else:\n",
    "                current_value += ' ' + item['word']  # Concatenate words for the same entity\n",
    "\n",
    "    # Append the last entity-value pair\n",
    "    if current_entity:\n",
    "        if current_value.find(':') != -1:\n",
    "            current_value = current_value.replace(' ', '')\n",
    "        state[current_entity].append(current_value)\n",
    "        \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need train reservations from norwich to cambridge\n",
      "{'train-destination': ['cambridge'], 'train-departure': ['norwich']}\n",
      "I have 133 trains matching your request. Is there a specific day and time you would like to travel?\n",
      "I'd like to leave on Monday and arrive by 18:00.\n",
      "{'train-destination': ['cambridge'], 'train-departure': ['norwich'], 'train-day': ['monday'], 'train-arriveBy': ['18:00']}\n",
      "There are 12 trains for the day and time you request.  Would you like to book it now?\n",
      "Before booking, I would also like to know the travel time, price, and departure time please.\n",
      "{'train-destination': ['cambridge'], 'train-departure': ['cambridge'], 'train-day': ['monday'], 'train-arriveBy': ['18:00']}\n",
      "There are 12 trains meeting your needs with the first leaving at 05:16 and the last one leaving at 16:16. Do you want to book one of these? \n",
      "No hold off on booking for now.  Can you help me find an attraction called cineworld cinema?\n",
      "{'train-destination': ['cambridge'], 'train-departure': ['cambridge'], 'train-day': ['monday'], 'train-arriveBy': ['18:00'], 'attraction-name': ['cineworld cinema']}\n",
      "Yes it is a cinema located in the south part of town what information would you like on it?\n",
      "Yes, that was all I needed. Thank you very much!\n",
      "{'train-destination': ['cambridge'], 'train-departure': ['cambridge'], 'train-day': ['monday'], 'train-arriveBy': ['18:00'], 'attraction-name': ['cineworld cinema']}\n",
      "Thank you for using our system.\n"
     ]
    }
   ],
   "source": [
    "state = {}\n",
    "max_turn = 3\n",
    "for turn in range(0, len(sample_text) - 1, 2):\n",
    "    history_text = '[SEP]'.join(history[max(0, turn - 2 * max_turn):turn])\n",
    "    full_text = f'{history_text}[SEP]{sample_text[turn]}' if history_text else sample_text[turn]\n",
    "    print(sample_text[turn])\n",
    "    state = dict(state | slot_vlaue_predict(full_text))\n",
    "    print(state)\n",
    "    print(sample_text[turn + 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
