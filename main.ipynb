{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, DataCollatorForTokenClassification, AutoModelForSequenceClassification, AutoModelForTokenClassification, Trainer, TrainingArguments, pipeline\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import evaluate\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/valListFile.json'\n",
    "test_list_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/testListFile.json'\n",
    "data_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/data.json'\n",
    "dialogue_acts_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/dialogue_acts.json'\n",
    "ontology_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/ontology.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đọc danh sách mã hội thoại val và test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(val_list_path) as f:\n",
    "    val_list = [line.strip() for line in f]\n",
    "    \n",
    "with open(test_list_path) as f:\n",
    "    test_list = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đọc dữ liệu hội thoại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path) as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "with open(dialogue_acts_path) as f:\n",
    "    dialogue_acts = json.load(f)\n",
    "    \n",
    "with open(ontology_path) as f:\n",
    "    ontology = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_ids = list(data.keys())\n",
    "\n",
    "train_list = [dialogue_id for dialogue_id in dialogue_ids if dialogue_id not in val_list and dialogue_id not in test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy tất cả các dialogue acts\n",
    "acts = set()\n",
    "\n",
    "for dialogue in dialogue_acts.values():\n",
    "    for turn in dialogue.values():\n",
    "        if turn == 'No Annotation':\n",
    "            continue\n",
    "        for act in turn.keys():\n",
    "            acts.add(act)\n",
    "            \n",
    "acts.add('No Annotation')\n",
    "acts = list(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng acts: 32\n",
      "['Taxi-Inform', 'Restaurant-NoOffer', 'Train-NoOffer', 'Attraction-Request', 'general-welcome', 'general-bye', 'Attraction-NoOffer', 'Booking-NoBook', 'Attraction-Select', 'general-greet', 'general-reqmore', 'Restaurant-Inform', 'Train-Inform', 'Restaurant-Request', 'Hotel-Request', 'Train-OfferBook', 'Hotel-Select', 'Attraction-Inform', 'Taxi-Request', 'Train-Select', 'Booking-Inform', 'Booking-Request', 'Train-OfferBooked', 'Hotel-NoOffer', 'Attraction-Recommend', 'No Annotation', 'Restaurant-Recommend', 'Hotel-Recommend', 'Restaurant-Select', 'Booking-Book', 'Train-Request', 'Hotel-Inform']\n"
     ]
    }
   ],
   "source": [
    "print('Số lượng acts:', len(acts))\n",
    "print(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo một từ điển chuyển đổi từ act sang index và ngược lại\n",
    "act2idx = {act: idx for idx, act in enumerate(acts)}\n",
    "idx2act = {idx: act for act, idx in act2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danh sách các slot dạng phân loại và không phân loại\n",
    "# Danh sách này được định nghĩa theo ý hiểu cá nhân\n",
    "# Với biến phân loại, ta sẽ chuyển thành one-hot vector\n",
    "# Với biến không phân loại, ta sẽ xác định span của nó trong câu\n",
    "\n",
    "categorical_slots = {\n",
    "    'attraction-area',\n",
    "    'attraction-type',\n",
    "    'bus-day',\n",
    "    'hotel-area',\n",
    "    'hotel-internet',\n",
    "    'hotel-parking',\n",
    "    'hotel-pricerange',\n",
    "    'hotel-stars',\n",
    "    'hotel-type',\n",
    "    'restaurant-area',\n",
    "    'restaurant-pricerange',\n",
    "    'train-departure',\n",
    "    'train-destination',\n",
    "}\n",
    "\n",
    "non_categorical_slots = {\n",
    "    'attraction-name',\n",
    "    'bus-arriveBy',\n",
    "    'bus-book people',\n",
    "    'bus-departure',\n",
    "    'bus-destination',\n",
    "    'bus-leaveAt',\n",
    "    'hospital-department',\n",
    "    'hotel-book day',\n",
    "    'hotel-book people',\n",
    "    'hotel-book stay',\n",
    "    'hotel-name',\n",
    "    'restaurant-book day',\n",
    "    'restaurant-book people',\n",
    "    'restaurant-book time',\n",
    "    'restaurant-food',\n",
    "    'restaurant-name',\n",
    "    'taxi-arriveBy',\n",
    "    'taxi-departure',\n",
    "    'taxi-destination',\n",
    "    'taxi-leaveAt',\n",
    "    'train-arriveBy',\n",
    "    'train-book people',\n",
    "    'train-day',\n",
    "    'train-leaveAt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng nhãn BIO: 49\n",
      "['O', 'B-taxi-arriveBy', 'I-taxi-arriveBy', 'B-bus-book people', 'I-bus-book people', 'B-bus-destination', 'I-bus-destination', 'B-restaurant-book day', 'I-restaurant-book day', 'B-taxi-leaveAt', 'I-taxi-leaveAt', 'B-restaurant-book people', 'I-restaurant-book people', 'B-taxi-destination', 'I-taxi-destination', 'B-bus-leaveAt', 'I-bus-leaveAt', 'B-restaurant-food', 'I-restaurant-food', 'B-bus-departure', 'I-bus-departure', 'B-train-book people', 'I-train-book people', 'B-hotel-name', 'I-hotel-name', 'B-restaurant-book time', 'I-restaurant-book time', 'B-hospital-department', 'I-hospital-department', 'B-train-arriveBy', 'I-train-arriveBy', 'B-train-day', 'I-train-day', 'B-train-leaveAt', 'I-train-leaveAt', 'B-hotel-book day', 'I-hotel-book day', 'B-taxi-departure', 'I-taxi-departure', 'B-restaurant-name', 'I-restaurant-name', 'B-hotel-book stay', 'I-hotel-book stay', 'B-bus-arriveBy', 'I-bus-arriveBy', 'B-attraction-name', 'I-attraction-name', 'B-hotel-book people', 'I-hotel-book people']\n"
     ]
    }
   ],
   "source": [
    "# Tạo nhãn BIO cho các slot không phân loại\n",
    "bio_list = ['O']\n",
    "bio_list.extend([item for slot in non_categorical_slots for item in [f'B-{slot}', f'I-{slot}']])\n",
    "print('Số lượng nhãn BIO:', len(bio_list))\n",
    "print(bio_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo một từ điển chuyển đổi từ nhãn BIO sang index và ngược lại\n",
    "bio2idx = {bio: idx for idx, bio in enumerate(bio_list)}\n",
    "idx2bio = {idx: bio for bio, idx in bio2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng nhãn value: 143\n",
      "['train-destination london kings cross', 'hotel-area north', 'attraction-type hiking', 'attraction-type churchills college', 'train-destination city centre north', 'attraction-type cinema', 'hotel-parking none', 'attraction-type museums', 'train-departure leicester', 'hotel-type guest house', 'train-destination dontcare', 'train-departure city hall', 'hotel-internet none', 'attraction-type historical', 'train-destination bournemouth', 'hotel-type guesthouse', 'hotel-type dontcare', 'attraction-type sports', 'attraction-type architecture', 'restaurant-area centre', 'attraction-type concert hall', 'hotel-internet no', 'hotel-internet yes', 'attraction-type museum', 'hotel-pricerange expensive', 'train-destination curry prince', 'train-destination norwich', 'attraction-type boat', 'train-destination broxbourne', 'hotel-area east', 'attraction-type entertainment', 'hotel-parking free', 'attraction-type camboats', 'hotel-internet dontcare', 'train-departure east london', 'train-destination glastonbury', 'attraction-area south', 'train-destination centre', 'train-departure stevenage', 'attraction-type park', 'attraction-type boating', 'train-destination london liverpool street', 'restaurant-pricerange dontcare', 'restaurant-area north', 'hotel-type hotel', 'hotel-pricerange none', 'train-departure london liverpool', 'attraction-type special', 'hotel-stars none', 'restaurant-pricerange cheap', 'train-departure london liverpool street', 'restaurant-pricerange none', 'restaurant-area west', 'attraction-area north', 'attraction-type cinemas', 'train-departure cineworld', 'attraction-type gallery', 'train-destination stansted airport', 'attraction-type none', 'attraction-type multiple sports', 'train-departure wandlebury country park', 'bus-day none', 'hotel-parking yes', 'hotel-stars 5', 'attraction-area centre', 'hotel-parking no', 'train-departure liverpool', 'train-destination none', 'attraction-type hotel', 'train-destination norway', 'train-departure aylesbray lodge guest', 'restaurant-area south', 'attraction-type church', 'hotel-type none', 'train-destination huntingdon marriott hotel', 'hotel-stars 0', 'hotel-type bed and breakfast', 'train-destination liverpool street', 'hotel-area none', 'train-departure ely', 'restaurant-area none', 'attraction-type night club', 'hotel-stars 4', 'train-departure dontcare', 'train-departure none', 'hotel-stars 1', 'train-destination ely', 'restaurant-pricerange expensive', 'train-departure london kings cross', 'hotel-parking dontcare', 'train-departure birmingham new street', 'train-departure peterborough', 'attraction-type concert', 'hotel-area dontcare', 'hotel-pricerange moderate', 'attraction-type theatre', 'train-departure brookshite', 'attraction-type concerthall', 'hotel-pricerange cheap', 'attraction-type dontcare', 'attraction-area east', 'hotel-area centre', 'train-destination kings lynn', 'restaurant-pricerange moderate', 'train-destination birmingham new street', 'attraction-type gastropub', 'attraction-area none', 'train-departure london', 'hotel-stars 3', 'train-departure kings lynn', 'bus-day wednesday', 'train-destination cambridge', 'train-destination peterborough', 'train-destination stevenage', 'train-destination leicester', 'hotel-pricerange dontcare', 'train-departure huntingdon', 'attraction-type college', 'hotel-stars dontcare', 'attraction-type swimming pool', 'attraction-area west', 'train-departure stratford', 'restaurant-area east', 'attraction-type museum kettles yard', 'train-departure duxford', 'attraction-type nightclub', 'train-departure broxbourne', 'train-departure stansted airport', 'hotel-area west', 'restaurant-area dontcare', 'attraction-type pool', 'attraction-type theater', 'train-destination liverpool', 'train-departure norwich', 'attraction-type theatres', 'train-departure cambridge', 'attraction-area dontcare', 'train-departure bishops stortford', 'train-destination london', 'train-departure panahar', 'hotel-stars 2', 'hotel-area south', 'train-destination bishops stortford']\n"
     ]
    }
   ],
   "source": [
    "# Tạo nhãn value cho các slot phân loại\n",
    "categorical_value_list = []\n",
    "\n",
    "for slot in categorical_slots:\n",
    "    for value in ontology[slot]:\n",
    "        for v in re.split(r'\\||>', value):\n",
    "            categorical_value_list.append(f'{slot} {v}')\n",
    "            \n",
    "categorical_value_list = list(set(categorical_value_list))\n",
    "print('Số lượng nhãn value:', len(categorical_value_list))\n",
    "print(categorical_value_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2idx = {cv: idx for idx, cv in enumerate(categorical_value_list)}\n",
    "idx2cv = {idx: cv for cv, idx in cv2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiWozDataset(Dataset):\n",
    "    def __init__(self, dialogue_data, acts_data, tokenizer, act2idx, bio2idx, cv2idx, max_turn=-1, dialogue_ids=None):\n",
    "        self.data = self._process_data(\n",
    "            dialogue_data, acts_data, max_turn, dialogue_ids)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.act2idx = act2idx\n",
    "        self.bio2idx = bio2idx\n",
    "        self.cv2idx = cv2idx\n",
    "        self.problem = 1 # 2, 3\n",
    "\n",
    "    def _process_data(self, dialogue_data, acts_data, max_turn, dialogue_ids):\n",
    "        data = []\n",
    "        for dialogue_id, dialogue in dialogue_data.items():\n",
    "            if dialogue_ids is not None and dialogue_id not in dialogue_ids:\n",
    "                continue\n",
    "            turns = dialogue['log']\n",
    "            history = []\n",
    "            for i in range(0, len(turns) - 1, 2):\n",
    "                user_turn = turns[i]\n",
    "                history.append(user_turn['text'])\n",
    "\n",
    "                system_turn = turns[i + 1]\n",
    "                history.append(system_turn['text'])\n",
    "\n",
    "                # Lấy act của system\n",
    "                system_acts = acts_data[dialogue_id[:-5]\n",
    "                                        ].get(str(i//2 + 1), 'No Annotation')\n",
    "                if system_acts == 'No Annotation':\n",
    "                    system_acts = ['No Annotation']\n",
    "                else:\n",
    "                    system_acts = list(system_acts.keys())\n",
    "\n",
    "                # Lấy slot, value của user\n",
    "                slot_values = []\n",
    "                for domain, domain_value in system_turn['metadata'].items():\n",
    "                    for slot, value in domain_value['book'].items():\n",
    "                        if slot == 'booked':\n",
    "                            continue\n",
    "                        if value and value != 'not mentioned':\n",
    "                            slot_values.append(\n",
    "                                [f'{domain}-book {slot}', value])\n",
    "                    for slot, value in domain_value['semi'].items():\n",
    "                        if value and value != 'not mentioned':\n",
    "                            slot_values.append([f'{domain}-{slot}', value])\n",
    "\n",
    "                data.append({\n",
    "                    'dialogue_id': dialogue_id,\n",
    "                    'history': history[max(0, i - 2 * max_turn):i] if max_turn > 0 else history[:i],\n",
    "                    'utterance': user_turn['text'],\n",
    "                    'system_acts': system_acts,\n",
    "                    'slot_values': slot_values\n",
    "                })\n",
    "        return data\n",
    "    \n",
    "    def set_problem(self, problem):\n",
    "        self.problem = problem\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "\n",
    "        history_text = '[SEP]'.join(data['history'])\n",
    "        full_text = f'{history_text}[SEP]{data[\"utterance\"]}' if history_text else data['utterance']\n",
    "\n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(full_text, return_tensors='pt')\n",
    "\n",
    "        if self.problem == 1:\n",
    "            act_labels = torch.zeros(len(self.act2idx))\n",
    "            for act in data['system_acts']:\n",
    "                act_labels[self.act2idx[act]] = 1\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'labels': act_labels\n",
    "            }\n",
    "\n",
    "        if self.problem == 2:\n",
    "            slot_labels = torch.tensor([self.bio2idx['O']] * len(encoding['input_ids'].squeeze(0)))\n",
    "            for slot, value in data['slot_values']:\n",
    "                if slot in non_categorical_slots:\n",
    "                    start, end = self._get_value_start_end(full_text, value)\n",
    "                    if start == 0 and end == 0:\n",
    "                        continue\n",
    "                    slot_labels[start] = self.bio2idx[f'B-{slot}']\n",
    "                    slot_labels[start + 1:end + 1] = self.bio2idx[f'I-{slot}']\n",
    "            # Chuyển label của [SEP] và [CLS] thành -100\n",
    "            sep_idx = (encoding['input_ids'] == self.tokenizer.sep_token_id).nonzero(as_tuple=True)[1]\n",
    "            cls_idx = (encoding['input_ids'] == self.tokenizer.cls_token_id).nonzero(as_tuple=True)[1]\n",
    "            slot_labels[sep_idx] = -100\n",
    "            slot_labels[cls_idx] = -100\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'labels': slot_labels\n",
    "            }\n",
    "            \n",
    "        if self.problem == 3:\n",
    "            categorical_labels = torch.zeros(len(self.cv2idx))\n",
    "            for slot, value in data['slot_values']:\n",
    "                if slot in categorical_slots:\n",
    "                    for v in re.split(r'\\||>', value):\n",
    "                        categorical_labels[self.cv2idx[f'{slot} {v}']] = 1\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'labels': categorical_labels\n",
    "            }\n",
    "\n",
    "    def _get_value_start_end(self, text, value):\n",
    "        tokenized_text = self.tokenizer.tokenize(text, add_special_tokens=True)\n",
    "        tokenized_value = self.tokenizer.tokenize(value)\n",
    "\n",
    "        start, end = 0, 0\n",
    "        for id_v, token_v in enumerate(tokenized_value):\n",
    "            for id_u, token_u in enumerate(tokenized_text):\n",
    "                if token_v == token_u:\n",
    "                    # nếu value được tìm thấy trong text\n",
    "                    if tokenized_value == tokenized_text[id_u:id_u+len(tokenized_value)]:\n",
    "                        start, end = id_u, id_u+len(tokenized_value) - 1\n",
    "                        break\n",
    "                    # nếu số lượng token còn lại trong text ít hơn số lượng token của value\n",
    "                    elif len(tokenized_text) - id_u + 1 <= len(tokenized_value):\n",
    "                        break\n",
    "        return torch.tensor([start, end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultiWozDataset(data, dialogue_acts, tokenizer, act2idx, bio2idx, cv2idx, max_turn=3, dialogue_ids=train_list)\n",
    "val_dataset = MultiWozDataset(data, dialogue_acts, tokenizer, act2idx, bio2idx, cv2idx, max_turn=3, dialogue_ids=val_list)\n",
    "test_dataset = MultiWozDataset(data, dialogue_acts, tokenizer, act2idx, bio2idx, cv2idx, max_turn=3, dialogue_ids=test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mô hình phát hiện system acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = sigmoid(predictions)\n",
    "    predictions = (predictions > 0.5).astype(int).reshape(-1)\n",
    "    return clf_metrics.compute(predictions=predictions, references=labels.astype(int).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/aircsrv5/miniconda3/envs/imlda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_12460/804699869.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "system_acts_model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = len(acts) , id2label=idx2act, label2id=act2idx, problem_type='multi_label_classification')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "   output_dir=\"model/system_acts_model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=3,\n",
    "   weight_decay=0.01,\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   save_strategy=\"epoch\",\n",
    "   load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "   model=system_acts_model,\n",
    "   args=training_args,\n",
    "   train_dataset=train_dataset,\n",
    "   eval_dataset=val_dataset,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10647' max='10647' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10647/10647 08:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.082800</td>\n",
       "      <td>0.078490</td>\n",
       "      <td>0.968381</td>\n",
       "      <td>0.610900</td>\n",
       "      <td>0.757795</td>\n",
       "      <td>0.511707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.077300</td>\n",
       "      <td>0.073625</td>\n",
       "      <td>0.970106</td>\n",
       "      <td>0.652718</td>\n",
       "      <td>0.747688</td>\n",
       "      <td>0.579154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.072800</td>\n",
       "      <td>0.072773</td>\n",
       "      <td>0.970475</td>\n",
       "      <td>0.660726</td>\n",
       "      <td>0.746397</td>\n",
       "      <td>0.592696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10647, training_loss=0.08612494631262355, metrics={'train_runtime': 495.9328, 'train_samples_per_second': 343.462, 'train_steps_per_second': 21.469, 'total_flos': 7296220874484864.0, 'train_loss': 0.08612494631262355, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='461' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/461 00:00 < 00:06, 76.15 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07327355444431305, 'eval_accuracy': 0.9703057175800326, 'eval_f1': 0.6580090807010692, 'eval_precision': 0.742344128662701, 'eval_recall': 0.5908811924594476, 'eval_runtime': 7.4223, 'eval_samples_per_second': 993.227, 'eval_steps_per_second': 62.11, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Thử nghiệm trên tập test\n",
    "print(trainer.evaluate(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_acts_classifier = pipeline('text-classification', model='model/system_acts_model', device=0, top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need train reservations from norwich to cambridge\n",
      "['Train-Request']\n",
      "I have 133 trains matching your request. Is there a specific day and time you would like to travel?\n",
      "I'd like to leave on Monday and arrive by 18:00.\n",
      "['Train-Request']\n",
      "There are 12 trains for the day and time you request.  Would you like to book it now?\n",
      "Before booking, I would also like to know the travel time, price, and departure time please.\n",
      "['Train-Inform']\n",
      "There are 12 trains meeting your needs with the first leaving at 05:16 and the last one leaving at 16:16. Do you want to book one of these? \n",
      "No hold off on booking for now.  Can you help me find an attraction called cineworld cinema?\n",
      "['Attraction-Inform']\n",
      "Yes it is a cinema located in the south part of town what information would you like on it?\n",
      "Yes, that was all I needed. Thank you very much!\n",
      "['general-bye']\n",
      "Thank you for using our system.\n"
     ]
    }
   ],
   "source": [
    "sample_text = [\n",
    "    \"I need train reservations from norwich to cambridge\",\n",
    "    \"I have 133 trains matching your request. Is there a specific day and time you would like to travel?\",\n",
    "    \"I'd like to leave on Monday and arrive by 18:00.\",\n",
    "    \"There are 12 trains for the day and time you request.  Would you like to book it now?\",\n",
    "    \"Before booking, I would also like to know the travel time, price, and departure time please.\",\n",
    "    \"There are 12 trains meeting your needs with the first leaving at 05:16 and the last one leaving at 16:16. Do you want to book one of these? \",\n",
    "    \"No hold off on booking for now.  Can you help me find an attraction called cineworld cinema?\",\n",
    "    \"Yes it is a cinema located in the south part of town what information would you like on it?\",\n",
    "    \"Yes, that was all I needed. Thank you very much!\",\n",
    "    \"Thank you for using our system.\"\n",
    "]\n",
    "\n",
    "history = []\n",
    "max_turn = 3\n",
    "threshold = 0.5\n",
    "for turn in range(0, len(sample_text) - 1, 2):\n",
    "    history_text = '[SEP]'.join(history[max(0, turn - 2 * max_turn):turn])\n",
    "    full_text = f'{history_text}[SEP]{sample_text[turn]}' if history_text else sample_text[turn]\n",
    "    print(sample_text[turn])\n",
    "    print([out['label'] for out in system_acts_classifier(full_text)[0] if out['score'] > threshold])\n",
    "    print(sample_text[turn + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mô hình phát hiện slot value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Với slot phân loại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_for_cv = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_problem(3)\n",
    "val_dataset.set_problem(3)\n",
    "test_dataset.set_problem(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/aircsrv5/miniconda3/envs/imlda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_12460/3317102083.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_cv = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17745' max='17745' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17745/17745 14:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.049110</td>\n",
       "      <td>0.986075</td>\n",
       "      <td>0.507678</td>\n",
       "      <td>0.805768</td>\n",
       "      <td>0.370582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.042797</td>\n",
       "      <td>0.989661</td>\n",
       "      <td>0.683100</td>\n",
       "      <td>0.840968</td>\n",
       "      <td>0.575135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>0.990417</td>\n",
       "      <td>0.713472</td>\n",
       "      <td>0.847948</td>\n",
       "      <td>0.615810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.043003</td>\n",
       "      <td>0.990563</td>\n",
       "      <td>0.719366</td>\n",
       "      <td>0.848626</td>\n",
       "      <td>0.624278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.043739</td>\n",
       "      <td>0.990640</td>\n",
       "      <td>0.722316</td>\n",
       "      <td>0.849345</td>\n",
       "      <td>0.628341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17745, training_loss=0.03500862968038525, metrics={'train_runtime': 853.5385, 'train_samples_per_second': 332.604, 'train_steps_per_second': 20.79, 'total_flos': 1.2178201492754736e+16, 'train_loss': 0.03500862968038525, 'epoch': 5.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_value_model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = len(categorical_value_list), id2label=idx2cv, label2id=cv2idx, problem_type='multi_label_classification')\n",
    "\n",
    "training_args_cv = TrainingArguments(\n",
    "   output_dir=\"model/categorical_value_model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=5,\n",
    "   weight_decay=0.01,\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   save_strategy=\"epoch\",\n",
    "   load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer_cv = Trainer(\n",
    "   model=categorical_value_model,\n",
    "   args=training_args_cv,\n",
    "   train_dataset=train_dataset,\n",
    "   eval_dataset=val_dataset,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator_for_cv,\n",
    "   compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_cv.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_cv.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='461' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/461 00:00 < 00:06, 75.56 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.03998860344290733,\n",
       " 'eval_accuracy': 0.9905036634553727,\n",
       " 'eval_f1': 0.7101537392512811,\n",
       " 'eval_precision': 0.8407486117776102,\n",
       " 'eval_recall': 0.6146752205292703,\n",
       " 'eval_runtime': 12.1907,\n",
       " 'eval_samples_per_second': 604.722,\n",
       " 'eval_steps_per_second': 37.816,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_cv.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Với slot không phân loại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_problem(2)\n",
    "val_dataset.set_problem(2)\n",
    "test_dataset.set_problem(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics_tf(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [idx2bio[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [idx2bio[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "non_categorical_value_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=len(bio_list), id2label=idx2bio, label2id=bio2idx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10647' max='10647' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10647/10647 10:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.022133</td>\n",
       "      <td>0.862717</td>\n",
       "      <td>0.882219</td>\n",
       "      <td>0.872359</td>\n",
       "      <td>0.992526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.019680</td>\n",
       "      <td>0.886945</td>\n",
       "      <td>0.901309</td>\n",
       "      <td>0.894069</td>\n",
       "      <td>0.993486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.019418</td>\n",
       "      <td>0.894676</td>\n",
       "      <td>0.911509</td>\n",
       "      <td>0.903014</td>\n",
       "      <td>0.993845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10647, training_loss=0.03067349930755452, metrics={'train_runtime': 607.2189, 'train_samples_per_second': 280.515, 'train_steps_per_second': 17.534, 'total_flos': 7198554702861252.0, 'train_loss': 0.03067349930755452, 'epoch': 3.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args_ncv = TrainingArguments(\n",
    "    output_dir=\"model/non_categorical_value_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer_ncv = Trainer(\n",
    "    model=non_categorical_value_model,\n",
    "    args=training_args_ncv,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics_tf,\n",
    ")\n",
    "\n",
    "trainer_ncv.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ncv.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='461' max='461' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [461/461 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.019307278096675873,\n",
       " 'eval_precision': 0.8939373389393734,\n",
       " 'eval_recall': 0.9118074289271633,\n",
       " 'eval_f1': 0.9027839605519979,\n",
       " 'eval_accuracy': 0.9940508945322825,\n",
       " 'eval_runtime': 14.9565,\n",
       " 'eval_samples_per_second': 492.897,\n",
       " 'eval_steps_per_second': 30.823,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ncv.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_value_clf = pipeline('text-classification', model='model/categorical_value_model', device=0, top_k=None)\n",
    "non_categorical_value_tclf = pipeline('ner', model='model/non_categorical_value_model', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slot_vlaue_predict(full_text):\n",
    "    state = defaultdict(list)\n",
    "    categorical_value_result = categorical_value_clf(full_text)\n",
    "    non_categorical_value_result = non_categorical_value_tclf(full_text)\n",
    "    \n",
    "    for out in categorical_value_result[0]:\n",
    "        if out['score'] > 0.5:\n",
    "            slot, value = out['label'].split(' ', 1)\n",
    "            state[slot].append(value)\n",
    "            \n",
    "    current_entity = None\n",
    "    current_value = \"\"\n",
    "    \n",
    "    for item in non_categorical_value_result:\n",
    "        entity_type = item['entity'][2:]  # Remove the B- or I- prefix\n",
    "        if item['entity'].startswith('B-'):\n",
    "            if current_entity:  # Save the previous entity-value pair if exists\n",
    "                if current_value.find(':') != -1:\n",
    "                    current_value = current_value.replace(' ', '')\n",
    "                state[current_entity].append(current_value)\n",
    "            current_entity = entity_type\n",
    "            current_value = item['word']\n",
    "        elif item['entity'].startswith('I-') and current_entity == entity_type:\n",
    "            if item['word'].startswith('##'):\n",
    "                current_value += item['word'][2:]\n",
    "            else:\n",
    "                current_value += ' ' + item['word']  # Concatenate words for the same entity\n",
    "\n",
    "    # Append the last entity-value pair\n",
    "    if current_entity:\n",
    "        if current_value.find(':') != -1:\n",
    "            current_value = current_value.replace(' ', '')\n",
    "        state[current_entity].append(current_value)\n",
    "        \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need train reservations from norwich to cambridge\n",
      "{'train-destination': ['cambridge'], 'train-departure': ['norwich']}\n",
      "I have 133 trains matching your request. Is there a specific day and time you would like to travel?\n",
      "I'd like to leave on Monday and arrive by 18:00.\n",
      "{'train-destination': ['cambridge'], 'train-departure': ['norwich'], 'train-day': ['monday'], 'train-arriveBy': ['18:00']}\n",
      "There are 12 trains for the day and time you request.  Would you like to book it now?\n",
      "Before booking, I would also like to know the travel time, price, and departure time please.\n",
      "{'train-destination': ['cambridge'], 'train-departure': ['norwich'], 'train-day': ['monday'], 'train-arriveBy': ['18:00']}\n",
      "There are 12 trains meeting your needs with the first leaving at 05:16 and the last one leaving at 16:16. Do you want to book one of these? \n",
      "No hold off on booking for now.  Can you help me find an attraction called cineworld cinema?\n",
      "{'train-destination': ['cambridge'], 'train-departure': ['norwich'], 'train-day': ['monday'], 'train-arriveBy': ['18:00'], 'attraction-name': ['cineworld cinema']}\n",
      "Yes it is a cinema located in the south part of town what information would you like on it?\n",
      "Yes, that was all I needed. Thank you very much!\n",
      "{'train-destination': ['cambridge'], 'train-departure': ['norwich'], 'train-day': ['monday'], 'train-arriveBy': ['18:00'], 'attraction-name': ['cineworld cinema']}\n",
      "Thank you for using our system.\n"
     ]
    }
   ],
   "source": [
    "state = {}\n",
    "for turn in range(0, len(sample_text) - 1, 2):\n",
    "    history_text = '[SEP]'.join(history[max(0, turn - 2 * max_turn):turn])\n",
    "    full_text = f'{history_text}[SEP]{sample_text[turn]}' if history_text else sample_text[turn]\n",
    "    print(sample_text[turn])\n",
    "    state = dict(state | slot_vlaue_predict(full_text))\n",
    "    print(state)\n",
    "    print(sample_text[turn + 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
