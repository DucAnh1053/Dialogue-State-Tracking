{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, DataCollatorForTokenClassification, AutoModelForSequenceClassification, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/valListFile.json'\n",
    "test_list_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/testListFile.json'\n",
    "data_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/data.json'\n",
    "dialogue_acts_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/dialogue_acts.json'\n",
    "ontology_path = 'MultiWOZ2.4-main/data/mwz24/MULTIWOZ2.4/ontology.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đọc danh sách mã hội thoại val và test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(val_list_path) as f:\n",
    "    val_list = [line.strip() for line in f]\n",
    "    \n",
    "with open(test_list_path) as f:\n",
    "    test_list = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đọc dữ liệu hội thoại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path) as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "with open(dialogue_acts_path) as f:\n",
    "    dialogue_acts = json.load(f)\n",
    "    \n",
    "with open(ontology_path) as f:\n",
    "    ontology = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_ids = list(data.keys())\n",
    "\n",
    "train_list = [dialogue_id for dialogue_id in dialogue_ids if dialogue_id not in val_list and dialogue_id not in test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy tất cả các dialogue acts\n",
    "acts = set()\n",
    "\n",
    "for dialogue in dialogue_acts.values():\n",
    "    for turn in dialogue.values():\n",
    "        if turn == 'No Annotation':\n",
    "            continue\n",
    "        for act in turn.keys():\n",
    "            acts.add(act)\n",
    "            \n",
    "acts.add('No Annotation')\n",
    "acts = list(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng acts: 32\n",
      "['Train-NoOffer', 'Attraction-Request', 'Taxi-Inform', 'Attraction-Recommend', 'Booking-Request', 'general-reqmore', 'Restaurant-Request', 'Booking-Inform', 'Attraction-NoOffer', 'Train-Request', 'Train-OfferBooked', 'Hotel-Recommend', 'Restaurant-Recommend', 'Taxi-Request', 'general-bye', 'Booking-NoBook', 'Hotel-Request', 'Train-Select', 'Restaurant-Inform', 'Hotel-Inform', 'Hotel-NoOffer', 'Restaurant-NoOffer', 'Train-OfferBook', 'Booking-Book', 'general-greet', 'Hotel-Select', 'No Annotation', 'Attraction-Select', 'Train-Inform', 'Attraction-Inform', 'general-welcome', 'Restaurant-Select']\n"
     ]
    }
   ],
   "source": [
    "print('Số lượng acts:', len(acts))\n",
    "print(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo một từ điển chuyển đổi từ act sang index và ngược lại\n",
    "act2idx = {act: idx for idx, act in enumerate(acts)}\n",
    "idx2act = {idx: act for act, idx in act2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danh sách các slot dạng phân loại và không phân loại\n",
    "# Danh sách này được định nghĩa theo ý hiểu cá nhân\n",
    "# Với biến phân loại, ta sẽ chuyển thành one-hot vector\n",
    "# Với biến không phân loại, ta sẽ xác định span của nó trong câu\n",
    "\n",
    "categorical_slots = {\n",
    "    'attraction-area',\n",
    "    'attraction-type',\n",
    "    'bus-day',\n",
    "    'hotel-area',\n",
    "    'hotel-internet',\n",
    "    'hotel-parking',\n",
    "    'hotel-pricerange',\n",
    "    'hotel-stars',\n",
    "    'hotel-type',\n",
    "    'restaurant-area',\n",
    "    'restaurant-pricerange',\n",
    "    'train-departure',\n",
    "    'train-destination',\n",
    "}\n",
    "\n",
    "non_categorical_slots = {\n",
    "    'attraction-name',\n",
    "    'bus-arriveBy',\n",
    "    'bus-book people',\n",
    "    'bus-departure',\n",
    "    'bus-destination',\n",
    "    'bus-leaveAt',\n",
    "    'hospital-department',\n",
    "    'hotel-book day',\n",
    "    'hotel-book people',\n",
    "    'hotel-book stay',\n",
    "    'hotel-name',\n",
    "    'restaurant-book day',\n",
    "    'restaurant-book people',\n",
    "    'restaurant-book time',\n",
    "    'restaurant-food',\n",
    "    'restaurant-name',\n",
    "    'taxi-arriveBy',\n",
    "    'taxi-departure',\n",
    "    'taxi-destination',\n",
    "    'taxi-leaveAt',\n",
    "    'train-arriveBy',\n",
    "    'train-book people',\n",
    "    'train-day',\n",
    "    'train-leaveAt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng nhãn BIO: 49\n",
      "['O', 'B-hotel-name', 'I-hotel-name', 'B-bus-destination', 'I-bus-destination', 'B-taxi-leaveAt', 'I-taxi-leaveAt', 'B-restaurant-book day', 'I-restaurant-book day', 'B-restaurant-food', 'I-restaurant-food', 'B-bus-leaveAt', 'I-bus-leaveAt', 'B-hotel-book day', 'I-hotel-book day', 'B-restaurant-book people', 'I-restaurant-book people', 'B-train-day', 'I-train-day', 'B-train-arriveBy', 'I-train-arriveBy', 'B-restaurant-name', 'I-restaurant-name', 'B-train-book people', 'I-train-book people', 'B-taxi-arriveBy', 'I-taxi-arriveBy', 'B-taxi-destination', 'I-taxi-destination', 'B-bus-arriveBy', 'I-bus-arriveBy', 'B-restaurant-book time', 'I-restaurant-book time', 'B-attraction-name', 'I-attraction-name', 'B-bus-departure', 'I-bus-departure', 'B-train-leaveAt', 'I-train-leaveAt', 'B-hotel-book people', 'I-hotel-book people', 'B-taxi-departure', 'I-taxi-departure', 'B-hospital-department', 'I-hospital-department', 'B-bus-book people', 'I-bus-book people', 'B-hotel-book stay', 'I-hotel-book stay']\n"
     ]
    }
   ],
   "source": [
    "# Tạo nhãn BIO cho các slot không phân loại\n",
    "bio_list = ['O']\n",
    "bio_list.extend([item for slot in non_categorical_slots for item in [f'B-{slot}', f'I-{slot}']])\n",
    "print('Số lượng nhãn BIO:', len(bio_list))\n",
    "print(bio_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo một từ điển chuyển đổi từ nhãn BIO sang index và ngược lại\n",
    "bio2idx = {bio: idx for idx, bio in enumerate(bio_list)}\n",
    "idx2bio = {idx: bio for bio, idx in bio2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng nhãn value: 143\n",
      "['restaurant-area dontcare', 'attraction-type nightclub', 'attraction-type boating', 'attraction-type hotel', 'hotel-pricerange moderate', 'hotel-area dontcare', 'attraction-type night club', 'attraction-type church', 'hotel-type none', 'attraction-type dontcare', 'train-destination cambridge', 'hotel-pricerange none', 'restaurant-pricerange dontcare', 'hotel-internet yes', 'train-destination city centre north', 'attraction-type park', 'attraction-type college', 'hotel-area east', 'train-departure city hall', 'attraction-type churchills college', 'train-departure none', 'train-destination stevenage', 'attraction-type sports', 'hotel-parking dontcare', 'attraction-type architecture', 'attraction-area west', 'train-departure brookshite', 'hotel-type hotel', 'train-departure stevenage', 'attraction-type museum', 'train-departure stratford', 'attraction-type cinemas', 'attraction-type theatre', 'hotel-stars 5', 'attraction-area south', 'attraction-type concert hall', 'train-destination bishops stortford', 'attraction-type hiking', 'train-departure liverpool', 'attraction-type special', 'hotel-internet no', 'hotel-pricerange dontcare', 'train-departure huntingdon', 'bus-day none', 'attraction-type gastropub', 'attraction-type museum kettles yard', 'train-departure cambridge', 'train-destination dontcare', 'train-departure wandlebury country park', 'hotel-stars 4', 'hotel-parking no', 'train-departure duxford', 'train-destination london kings cross', 'train-departure london', 'train-departure dontcare', 'restaurant-area east', 'attraction-type camboats', 'train-destination kings lynn', 'train-departure bishops stortford', 'train-destination norway', 'train-departure broxbourne', 'attraction-area east', 'restaurant-area south', 'hotel-type guesthouse', 'hotel-parking free', 'hotel-area west', 'train-departure panahar', 'train-destination london liverpool street', 'restaurant-pricerange cheap', 'hotel-pricerange expensive', 'attraction-type entertainment', 'attraction-type historical', 'attraction-type theater', 'train-destination broxbourne', 'attraction-area north', 'attraction-type gallery', 'hotel-type dontcare', 'hotel-stars 0', 'train-destination glastonbury', 'attraction-type concerthall', 'restaurant-pricerange none', 'train-destination huntingdon marriott hotel', 'train-departure london kings cross', 'train-destination bournemouth', 'train-departure stansted airport', 'train-destination leicester', 'train-destination liverpool street', 'train-departure london liverpool', 'train-destination norwich', 'attraction-area centre', 'restaurant-area west', 'train-departure leicester', 'hotel-type bed and breakfast', 'attraction-type theatres', 'train-departure peterborough', 'attraction-type concert', 'train-destination london', 'hotel-internet dontcare', 'hotel-area south', 'train-destination centre', 'train-departure kings lynn', 'attraction-area none', 'train-departure birmingham new street', 'hotel-stars 3', 'restaurant-area none', 'attraction-type cinema', 'attraction-type swimming pool', 'train-departure ely', 'attraction-type none', 'train-destination liverpool', 'restaurant-pricerange moderate', 'hotel-stars 1', 'bus-day wednesday', 'train-departure london liverpool street', 'hotel-type guest house', 'train-departure east london', 'train-departure cineworld', 'restaurant-area north', 'hotel-area none', 'attraction-area dontcare', 'restaurant-area centre', 'hotel-parking yes', 'hotel-stars none', 'train-destination ely', 'train-destination none', 'train-destination curry prince', 'attraction-type pool', 'train-departure norwich', 'train-destination birmingham new street', 'hotel-area centre', 'hotel-parking none', 'restaurant-pricerange expensive', 'train-destination peterborough', 'attraction-type museums', 'train-departure aylesbray lodge guest', 'hotel-internet none', 'attraction-type multiple sports', 'hotel-stars 2', 'train-destination stansted airport', 'attraction-type boat', 'hotel-pricerange cheap', 'hotel-stars dontcare', 'hotel-area north']\n"
     ]
    }
   ],
   "source": [
    "# Tạo nhãn value cho các slot phân loại\n",
    "categorical_value_list = []\n",
    "\n",
    "for slot in categorical_slots:\n",
    "    for value in ontology[slot]:\n",
    "        for v in re.split(r'\\||>', value):\n",
    "            categorical_value_list.append(f'{slot} {v}')\n",
    "            \n",
    "categorical_value_list = list(set(categorical_value_list))\n",
    "print('Số lượng nhãn value:', len(categorical_value_list))\n",
    "print(categorical_value_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2idx = {cv: idx for idx, cv in enumerate(categorical_value_list)}\n",
    "idx2cv = {idx: cv for cv, idx in cv2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiWozDataset(Dataset):\n",
    "    def __init__(self, dialogue_data, acts_data, tokenizer, act2idx, bio2idx, cv2idx, max_turn=-1, dialogue_ids=None):\n",
    "        self.data = self._process_data(\n",
    "            dialogue_data, acts_data, max_turn, dialogue_ids)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.act2idx = act2idx\n",
    "        self.bio2idx = bio2idx\n",
    "        self.cv2idx = cv2idx\n",
    "        self.problem = 1 # 2, 3\n",
    "\n",
    "    def _process_data(self, dialogue_data, acts_data, max_turn, dialogue_ids):\n",
    "        data = []\n",
    "        for dialogue_id, dialogue in dialogue_data.items():\n",
    "            if dialogue_ids is not None and dialogue_id not in dialogue_ids:\n",
    "                continue\n",
    "            turns = dialogue['log']\n",
    "            history = []\n",
    "            for i in range(0, len(turns) - 1, 2):\n",
    "                user_turn = turns[i]\n",
    "                history.append(user_turn['text'])\n",
    "\n",
    "                system_turn = turns[i + 1]\n",
    "                history.append(system_turn['text'])\n",
    "\n",
    "                # Lấy act của system\n",
    "                system_acts = acts_data[dialogue_id[:-5]\n",
    "                                        ].get(str(i//2 + 1), 'No Annotation')\n",
    "                if system_acts == 'No Annotation':\n",
    "                    system_acts = ['No Annotation']\n",
    "                else:\n",
    "                    system_acts = list(system_acts.keys())\n",
    "\n",
    "                # Lấy slot, value của user\n",
    "                slot_values = []\n",
    "                for domain, domain_value in system_turn['metadata'].items():\n",
    "                    for slot, value in domain_value['book'].items():\n",
    "                        if slot == 'booked':\n",
    "                            continue\n",
    "                        if value and value != 'not mentioned':\n",
    "                            slot_values.append(\n",
    "                                [f'{domain}-book {slot}', value])\n",
    "                    for slot, value in domain_value['semi'].items():\n",
    "                        if value and value != 'not mentioned':\n",
    "                            slot_values.append([f'{domain}-{slot}', value])\n",
    "\n",
    "                data.append({\n",
    "                    'dialogue_id': dialogue_id,\n",
    "                    'history': history[max(0, i - 2 * max_turn):i] if max_turn > 0 else history[:i],\n",
    "                    'utterance': user_turn['text'],\n",
    "                    'system_acts': system_acts,\n",
    "                    'slot_values': slot_values\n",
    "                })\n",
    "        return data\n",
    "    \n",
    "    def set_problem(self, problem):\n",
    "        self.problem = problem\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "\n",
    "        history_text = '[SEP]'.join(data['history'])\n",
    "        full_text = f'{history_text}[SEP]{data[\"utterance\"]}' if history_text else data['utterance']\n",
    "\n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(full_text, return_tensors='pt')\n",
    "\n",
    "        if self.problem == 1:\n",
    "            act_labels = torch.zeros(len(self.act2idx))\n",
    "            for act in data['system_acts']:\n",
    "                act_labels[self.act2idx[act]] = 1\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'labels': act_labels\n",
    "            }\n",
    "\n",
    "        if self.problem == 2:\n",
    "            slot_labels = torch.tensor([self.bio2idx['O']] * len(encoding['input_ids'].squeeze(0)))\n",
    "            for slot, value in data['slot_values']:\n",
    "                if slot in non_categorical_slots:\n",
    "                    start, end = self._get_value_start_end(full_text, value)\n",
    "                    if start == 0 and end == 0:\n",
    "                        continue\n",
    "                    slot_labels[start] = self.bio2idx[f'B-{slot}']\n",
    "                    slot_labels[start + 1:end + 1] = self.bio2idx[f'I-{slot}']\n",
    "            # Chuyển label của [SEP] và [CLS] thành -100\n",
    "            sep_idx = (encoding['input_ids'] == self.tokenizer.sep_token_id).nonzero(as_tuple=True)[1]\n",
    "            cls_idx = (encoding['input_ids'] == self.tokenizer.cls_token_id).nonzero(as_tuple=True)[1]\n",
    "            slot_labels[sep_idx] = -100\n",
    "            slot_labels[cls_idx] = -100\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'labels': slot_labels\n",
    "            }\n",
    "            \n",
    "        if self.problem == 3:\n",
    "            categorical_labels = torch.zeros(len(self.cv2idx))\n",
    "            for slot, value in data['slot_values']:\n",
    "                if slot in categorical_slots:\n",
    "                    categorical_labels[self.cv2idx[f'{slot} {value}']] = 1\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'labels': categorical_labels\n",
    "            }\n",
    "\n",
    "    def _get_value_start_end(self, text, value):\n",
    "        tokenized_text = self.tokenizer.tokenize(text, add_special_tokens=True)\n",
    "        tokenized_value = self.tokenizer.tokenize(value)\n",
    "\n",
    "        start, end = 0, 0\n",
    "        for id_v, token_v in enumerate(tokenized_value):\n",
    "            for id_u, token_u in enumerate(tokenized_text):\n",
    "                if token_v == token_u:\n",
    "                    # nếu value được tìm thấy trong text\n",
    "                    if tokenized_value == tokenized_text[id_u:id_u+len(tokenized_value)]:\n",
    "                        start, end = id_u, id_u+len(tokenized_value) - 1\n",
    "                        break\n",
    "                    # nếu số lượng token còn lại trong text ít hơn số lượng token của value\n",
    "                    elif len(tokenized_text) - id_u + 1 <= len(tokenized_value):\n",
    "                        break\n",
    "        return torch.tensor([start, end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultiWozDataset(data, dialogue_acts, tokenizer, act2idx, bio2idx, cv2idx, dialogue_ids=train_list)\n",
    "val_dataset = MultiWozDataset(data, dialogue_acts, tokenizer, act2idx, bio2idx, cv2idx, dialogue_ids=val_list)\n",
    "test_dataset = MultiWozDataset(data, dialogue_acts, tokenizer, act2idx, bio2idx, cv2idx, dialogue_ids=test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "data_collator_for_tc = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mô hình phát hiện system acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = sigmoid(predictions)\n",
    "    predictions = (predictions > 0.5).astype(int).reshape(-1)\n",
    "    return clf_metrics.compute(predictions=predictions, references=labels.astype(int).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 66978080\n"
     ]
    }
   ],
   "source": [
    "system_acts_model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', id2label=idx2act, label2id=act2idx, problem_type='multi_label_classification')\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#    output_dir=\"model\",\n",
    "#    learning_rate=2e-5,\n",
    "#    per_device_train_batch_size=16,\n",
    "#    per_device_eval_batch_size=16,\n",
    "#    num_train_epochs=3,\n",
    "#    weight_decay=0.01,\n",
    "#    evaluation_strategy=\"epoch\",\n",
    "#    save_strategy=\"epoch\",\n",
    "#    load_best_model_at_end=True,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#    model=system_acts_model,\n",
    "#    args=training_args,\n",
    "#    train_dataset=train_dataset,\n",
    "#    eval_dataset=val_dataset,\n",
    "#    tokenizer=tokenizer,\n",
    "#    data_collator=data_collator,\n",
    "#    compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "total_params = sum(p.numel() for p in system_acts_model.parameters())\n",
    "print(f'Total number of parameters: {total_params}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
